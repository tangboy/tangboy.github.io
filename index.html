<!DOCTYPE html>
<html lang="zh-cn">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="机器学习，编程，玩">













  <link rel="alternate" href="/default" title="Anthon">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.10.1">



<link rel="canonical" href="http://code-monkey.top/">



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css">




  <link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css">



<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.10.1">



  



  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>









<script>
  window.config = {"leancloud":{"app_id":null,"app_key":null},"toc":true,"fancybox":true,"pjax":true};
</script>

    <title> Anthon </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/." class="logo">Anthon</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            标签
          
        </li>
      </a>
    
      <a href="/about">
        <li class="mobile-menu-item">
          
          
            关于
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Anthon</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/about">
            
            
              关于
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <section id="posts" class="posts">
    
      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/02/28/埋点数据/">埋点数据(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-02-28
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        

        
          <img src="/2019/02/28/埋点数据/1.webp">
<h2 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h2><p>埋点作为商业智能（BI）和人工智能（AI）体系中重要的一环，是公司提升产品工程质量、实施 AB Testing、个性化推荐服务重要的数据来源。在传统的纯 Web 和 Native 开发的产品中，埋点从技术的角度来说未必多深奥，但从业务的角度来说要做到埋点设计规范、流程高效和保证质量却是很难。本文重点介绍一下知乎客户端的埋点模型、流程和平台技术。</p>
<h2 id="客户端埋点为什么难？"><a href="#客户端埋点为什么难？" class="headerlink" title="客户端埋点为什么难？"></a>客户端埋点为什么难？</h2><p>Web 端的埋点可以随着新代码上线即时生效，对版本的发车概念相对较弱，即使埋点错漏，修复成本较低。</p>
<p>对客户端而言，如果使用 Native 技术开发的功能埋点有问题，则需要等下一个版本才能修复，并且还有版本覆盖度的问题。修复埋点的这个时间窗口一般都比较长，会对业务的产品快速迭代产生很负面的影响。从业务的角度来说，客户端在发布功能之前，对于要做的数据分析不见得想得全，无计划收集非常多的埋点，对于埋点设计人员、客户端开发、测试人员来说是很大的工作量。反过来说，真正要用数时才发现重要的埋点没有采集，则会 「点」 到用时方恨少。因此，如何综合规划一个版本要采集的埋点，也是颇有挑战的事情，颇有「养兵千日，用兵一时」的感觉。</p>
<h2 id="埋点的流程"><a href="#埋点的流程" class="headerlink" title="埋点的流程"></a>埋点的流程</h2><p>从业务过程中采集埋点，是数据驱动型公司的必要条件。知乎的产品功能评审环节，不仅有 PRD (Product requirement document），还加入了对应的 DRD ( Data requirement document）。对于埋点而言，DRD 需要明确业务目标与埋点缺口之间的关系以及需求的优先级。埋点的需求大多来自于 DRD，整个过程会涉及多个角色，主要包括产品经理、业务数据负责人、开发工程师、测试工程师。</p>
<img src="/2019/02/28/埋点数据/2.webp">
<p>回顾知乎埋点流程的迭代史，整个流程落地三部曲可以总结为六个字：能力、意愿、工具。</p>
<h3 id="能力"><a href="#能力" class="headerlink" title="能力"></a>能力</h3><p>这几年知乎的业务发展很快，埋点的流程也随着迭代了很多个版本。在数据平台组成立之初就研发了全端埋点 SDK 和日志的接收服务。在有了埋点 SDK 之后，数据平台组开始在公司推广埋点工作，在早期是埋点的推动方和设计者，使得公司基本具备了打点的能力。</p>
<h3 id="意愿"><a href="#意愿" class="headerlink" title="意愿"></a>意愿</h3><p>为了快速推进业务的埋点，数据平台组招聘了埋点设计人员来设计全公司的打点。这个方法在短期内帮助公司的埋点工作顺利进行，但是很快随着业务持续的增长，即使是埋点设计的老手也无法快速响应业务的埋点需求，跨业务的任务排期也给业务带来较多的困扰。我们发现埋点的流程如果做到业务闭环，能让整个流程变得更为高效和顺利。业务中哪个角色更有意愿来设计埋点是流程是否高效的重要因素。以下是业务几个和数据有关角色的主要工作内容：</p>
<ul>
<li>数据分析师和产品经理主要是数据的使用者，工作内容是发现和解决业务的问题，不断对产品进行迭代</li>
<li>工程师对代码的细节和打点时机最为了解，但是对于数据具体的使用不见得很清晰</li>
<li>据仓库接口人负责业务数据的生产，和数据仓库团队对接，对埋点的定义需要有深入的理解综合考虑各角色的意愿后，我们设计了「业务数据负责人」这个角色，来整体来负责业务的数据生产工作，主要负责业务数据仓库需求和埋点设计。</li>
</ul>
<h3 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h3><p>早期埋点测试只有一个能力有限的小工具，用户体验并不够好，直接将埋点测试作为客户端发版流程中的一部分只会整体降低测试工程师的效率。客户端发版往往会遇到新增的埋点打重、打错和打漏，老的埋点缺少回归测试等等问题，给业务带来了不少困扰。因此一个易用性高、自动化和智能化的埋点测试平台成了当时迫在眉睫的事情。在开发完一整套埋点管理和测试系统后，测试工程师将埋点加入了客户端发版流程，并对全公司埋点做了整体评审，推进业务完善了埋点的元信息，并对核心埋点创建了回归测试。在埋点测试平台有效使用起来之后，埋点的质量相比之前得到了大幅度的提升。</p>
<h2 id="埋点的模型"><a href="#埋点的模型" class="headerlink" title="埋点的模型"></a>埋点的模型</h2><p>古语有云：「治大国若烹小鲜」。目前知乎的埋点数量约为三千个，如果缺少统一的模型来做标准化，每个人设计出来的埋点都不一样。数据平台为此提供公司级通用的埋点模型，既要有公司级别的规范，又要满足业务个性化的需求。在技术上，我们使用 Protocol Buffers 管理埋点 Schema，统一埋点字段和 enum 类型取值，统一 SDK 发版。</p>
<h3 id="页面浏览"><a href="#页面浏览" class="headerlink" title="页面浏览"></a>页面浏览</h3><p>页面浏览的统计，对于 Web 端而言， 因为 URL 非常明确， 统计规则简单清新。通常来说，根据一些正则对 URL 进行分类，即可统计出某类页面的 PV。</p>
<p>对于客户端而言，统计的方式和 Web 端比较相似。由于客户端不像 Web 端天然具备 URL，因此需要为页面伪造 URL。只要能被定义 URL，那么 URL 变化了，即可算一次新的 PV。客户端页面浏览统计中，我们遇到的最难的问题是：页面是什么？如果说页面的跳转算一次新的曝光，问题在于页面的功能变化多少算一次页面的跳转？一个典型的场景是一个页面中某子模块进行了 Tab 间切换时，当前页面的 PV 该如何统计。目前对于这个问题，知乎目前没有做统一，由业务自己来定义。</p>
<h3 id="行为事件"><a href="#行为事件" class="headerlink" title="行为事件"></a>行为事件</h3><p>对于行为事件，知乎选择了事件模型，完整描述 Who、When、Where、How 和 What 五大要素。</p>
<p>Who、When 和 How</p>
<ul>
<li>Who：用户和设备的身份特征。</li>
<li>When：埋点触发的时间。</li>
<li><p>How：埋点发生时，用户当前的状态，例如网络是 4G 还是 Wifi，当前的 AB 实验命中情况等等。模型中 Who、When、How 由埋点 SDK 自动生成，埋点人员在绝大多数情况下不必关心这三个要素。</p>
</li>
<li><p>Where 准确定位一个事件发生的位置。主要包含以下几个字段提供埋点设计者来做用户事件的定位。</p>
<img src="/2019/02/28/埋点数据/3.webp"></li>
<li>What 在事件发生位置上的内容信息，这里采集的内容由业务决定。 例如点击的卡片是一个回答还是一个 Live，当前内容的状态这类需求。</li>
</ul>
<p>对于业务定制化的「What」，最初我们为个性化的需求，设计了通用的 ContentInfo，以及特定领域的数据结构。<br><img src="/2019/02/28/埋点数据/4.webp"></p>
<p>对于 What，在客户端开发上，我们主要遇到以下问题：</p>
<ol>
<li>采集需要的数据有时和客户端功能开发无关，客户端获取数据难</li>
<li>当数据结构较复杂，客户端工作量增大</li>
<li>打错和打漏的情况，需要发版，周期长面对上述打点，对于不是必须由客户端获取的数据改成由业务后端生成 Protocol Buffers 结构，序列化成 string 随 api 带回客户端，客户端只需将 string 放置到通用的位置即可。数据平台组统一的实时 ETL 程序会反序列化该结构，过程如下图所示。</li>
</ol>
<img src="/2019/02/28/埋点数据/5.webp">
<p>对于 What，在埋点设计上，目前主要遇到以下问题：</p>
<ul>
<li>埋点的 Key 越来越多，字段和业务并没有在系统级别绑定关系，有些字段多个业务在用，枚举值越来越多，对埋点设计者造成了较多的信息噪音</li>
<li>业务依赖了其他业务的打点，埋点变更可能导致其他业务的核心指标受到影响</li>
</ul>
<p>第一个问题我们正在对埋点字段进行治理，将平台通用字段和业务字段做系统级别的元信息完善。第二个问题，我们目前还在探索中</p>
<h2 id="埋点的平台技术"><a href="#埋点的平台技术" class="headerlink" title="埋点的平台技术"></a>埋点的平台技术</h2><h3 id="埋点管理平台"><a href="#埋点管理平台" class="headerlink" title="埋点管理平台"></a>埋点管理平台</h3><p>当公司的规模生态还很小时，埋点使用 Excel 或者 Wiki 管理对埋点使用上影响不大。当公司业务快速发展，从一个产品变成多个产品，从几十个埋点变成几千个埋点，想要精准的用好埋点，就需要开发埋点的管理平台了。</p>
<p>埋点管理平台负责管理埋点的元信息，解决了埋点的录入和查找需求，同时简化了客户端埋点的内容， 是知乎埋点流程的重要组成部分。同时在工程上又为埋点测试平台，数据采集系统提供埋点的元信息接口。</p>
<h4 id="查看埋点"><a href="#查看埋点" class="headerlink" title="查看埋点"></a>查看埋点</h4><p>支持按照多个标签来查找和过滤埋点。 在创建埋点时，需要花时间录入这些元信息，从长期来看，收益会非常大。</p>
<img src="/2019/02/28/埋点数据/6.webp">
<h4 id="创建埋点"><a href="#创建埋点" class="headerlink" title="创建埋点"></a>创建埋点</h4><p>在创建埋点时，填写埋点对应的业务元信息和技术元信息，包括埋点对应的测试说明。埋点管理平台提供埋点的 key，如果需要新增 key 则可向平台申请。对于 enum 类型的 value，系统会自动补全。</p>
<h4 id="生成埋点设计文档"><a href="#生成埋点设计文档" class="headerlink" title="生成埋点设计文档"></a>生成埋点设计文档</h4><p>埋点设计文档是工程师开发埋点的依据，是埋点流程中交流需要的重要「媒介」。埋点文档标准化了埋点的设计，包含埋点的以下信息：</p>
<ol>
<li>埋点的基本信息：业务、等级、应用、使用说明、打点时机、测试说明、需求文档等</li>
<li>埋点对应的角色：数据负责人、开发、QA</li>
<li>埋点对应的字段和字段的取值</li>
</ol>
<h4 id="提供埋点元信息-API"><a href="#提供埋点元信息-API" class="headerlink" title="提供埋点元信息 API"></a>提供埋点元信息 API</h4><p>数据采集服务会对采集到的埋点写入到 Kafka 中，对于各个业务的实时数据消费需求，我们为每个业务提供了单独的 Kafka，流量分发模块会定期读取埋点管理平台提供的元信息，将流量实时分发的各业务 Kafka 中。</p>
<img src="/2019/02/28/埋点数据/7.webp">
<h3 id="埋点测试平台"><a href="#埋点测试平台" class="headerlink" title="埋点测试平台"></a>埋点测试平台</h3><p>埋点的质量是数据的生命线，一旦出现问题，则会导致整条大数据链路的数据价值出现问题。埋点异常不但影响决策，修复数据同样会消耗大量的精力和时间，最直接的后果就是虽然数据量越来越大，数据本身却无法有效的使用。知乎的数据团队在 2016 年做了一个埋点的小工具，只要输入测试设备的 id，就可以查看对应的埋点信息。这个工具主要有以下几个痛点：</p>
<ul>
<li>埋点日志量大，通常很难找到自己想测试的埋点</li>
<li>展示一整条日志，系统无法判定埋点是否准确，全靠肉眼来看</li>
<li>无法创建测试用例，不能做回归测试</li>
<li>埋点漏了或者错了人力尚能发现，埋点重复发送人很难发现</li>
</ul>
<p>面对如上问题，我们重新设计了埋点测试平台，目标是让埋点测试更自动化和智能化，主要有以下功能：</p>
<ul>
<li>可创建埋点测试用例，打通埋点管理平台，支持多条件筛选埋点</li>
<li>支持发起埋点测试实例，只展示埋点测试用例中的埋点，多余信息单独展示</li>
<li>自动化提示埋点打错、打漏和打重，前端界面高亮展示，生成测试报告</li>
<li>支持手机扫码连上系统，无需人输设备 id</li>
</ul>
<img src="/2019/02/28/埋点数据/8.webp">
<h2 id="其他：关于-Hybrid-类型埋点"><a href="#其他：关于-Hybrid-类型埋点" class="headerlink" title="其他：关于 Hybrid 类型埋点"></a>其他：关于 Hybrid 类型埋点</h2><p>客户端内的 H5 生成埋点使用的是 JavaScript SDK，如果直接发送到日志收集服务，会丢失客户端的重要属性。知乎的做法是将 H5 的日志发送给客户端，由客户端处理后发送给日志接收服务。在知乎我们对 H5 这类统称 Hybrid，我们自研了 Hybrid 框架，跨端通信和埋点传输由框架提供支持，自动化解决和 ZA （Zhihu Analytics Log Server）的通信问题。</p>
<img src="/2019/02/28/埋点数据/9.webp">
<p>Hybrid 框架主要处理以下的问题：</p>
<ul>
<li>对于 Native 和 JS 混合的页面，该页面曝光统计</li>
<li>对于 JS 页面内部的跳转，页面曝光的统计</li>
<li>JS SDK 生成的日志，传输到 Native，并发送给日志收集服务</li>
<li>对于 UTM 系列追踪链，做到跨 Native 和 JS 支持</li>
</ul>
<h2 id="总-结"><a href="#总-结" class="headerlink" title="总  结"></a>总  结</h2><p>今天的大数据发展趋势之快，对于很多公司来说都是挑战，埋点是数据整个数据链路中的起点，是数据的生命之源。随着知乎的快速发展，业务越来越多，知乎的埋点模型、流程和平台技术在不断迭代当中，在应用实践上还有很大的改进的空间。</p>

        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/01/19/Chaining-Custom-PySpark-DataFrame-Transformations/">Chaining Custom PySpark DataFrame Transformations</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-01-19
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <p>和基于Scala Spark API一样，我们也期望在Pyspark中也能够将不同的transformation方法连接起来统一执行。</p>
<p>这篇博客主要描述如何给Pyspark DataFrame添加<code>transform</code>方法，从而支持能够将自定义的DataFrame transformation连接起来。</p>
<p>同时，我们也会介绍如何利用<a href="https://github.com/pytoolz/cytoolz" target="_blank" rel="noopener">cytoolz</a>来批量顺序执行自定义变换函数</p>
<h2 id="Chaining-DataFrame-Transformations-with-Lambda"><a href="#Chaining-DataFrame-Transformations-with-Lambda" class="headerlink" title="Chaining DataFrame Transformations with Lambda"></a>Chaining DataFrame Transformations with Lambda</h2><p>首先，在原生的Pyspark DataFrame增加<code>transform</code>方法，从而我们能够串联执行DataFrame变换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.dataframe <span class="keyword">import</span> DataFrame</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, f)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> f(self)</span><br><span class="line">    </span><br><span class="line">DataFrame.transform = transform</span><br></pre></td></tr></table></figure>
<p>接下来，我们定义一些简单的DataFrame变换方法来测试<code>transform</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">with_greeting</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> df.withColumn(<span class="string">"greeting"</span>, lit(<span class="string">"hi"</span>))</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">with_something</span><span class="params">(df, something)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> df.withColumn(<span class="string">"something"</span>, lit(something))</span><br></pre></td></tr></table></figure>
<p>创建一个DataFrame然后串联执行<code>with_greeting</code>和<code>with_something</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">data = [(<span class="string">"jsoe"</span>, <span class="number">1</span>), (<span class="string">"li"</span>, <span class="number">2</span>), (<span class="string">"liz"</span>, <span class="number">3</span>)]</span><br><span class="line">source_df = spark.createDataFrame(data, [<span class="string">"name"</span>, <span class="string">"age"</span>])</span><br><span class="line"></span><br><span class="line">actual_df = (source_df</span><br><span class="line">                .transform(<span class="keyword">lambda</span> df: with_greeting(df))</span><br><span class="line">                .transform(<span class="keyword">lambda</span> df: with_something(df, <span class="string">"crazy"</span>)))</span><br><span class="line">                </span><br><span class="line">print(actual_df.show())</span><br><span class="line"></span><br><span class="line">+----+---+--------+---------+</span><br><span class="line">|name|age|greeting|something|</span><br><span class="line">+----+---+--------+---------+</span><br><span class="line">|jose|  <span class="number">1</span>|      hi|    crazy|</span><br><span class="line">|  li|  <span class="number">2</span>|      hi|    crazy|</span><br><span class="line">| liz|  <span class="number">3</span>|      hi|    crazy|</span><br><span class="line">+----+---+--------+---------+</span><br></pre></td></tr></table></figure>
<p>对于只有一个DataFrame参数的自定义变换中，<code>lambda</code>是可以省略的，从而我们可以简化调用方式如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">actual_df = (source_df</span><br><span class="line">             .transform(with_greeting)</span><br><span class="line">             .transform(<span class="keyword">lambda</span> df: with_something(df, <span class="string">"crazy"</span>)))</span><br></pre></td></tr></table></figure>
<p>如果我们没有定义<code>DataFrame#transform</code>方法，我们就不需要像下面的代码一样，来调用不同的transformation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1 = with_greeting(source_df)</span><br><span class="line">actual_df = with_something(df1, <span class="string">"moo"</span>)</span><br></pre></td></tr></table></figure>
<p>比较上述代码，采用<code>transform</code>来调用不同的DataFrame的变化，能够避免定义中间DataFrame，从而使得代码更加清晰</p>
<p>接下来，我们进一步探讨transformations的其他定义形式，让整个<code>transform</code>更加清晰明了</p>
          <div class="read-more">
            <a href="/2019/01/19/Chaining-Custom-PySpark-DataFrame-Transformations/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/01/19/Chaining-Custom-DataFrame-Transformations-in-Spark/">Chaining Custom DataFrame Transformations in Spark</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-01-19
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <p>在Spark中，可以采用<code>implicit classes</code>或者<code>Dataset#transform</code>来连接DataFrame的变换，这篇博客着重描如何连接DataFrame变换操作，并且详细解释说明为啥<code>Dataset#transform</code>方式要比<code>implicit classes</code>更有优势。</p>
<h2 id="Dataset-Transform方法"><a href="#Dataset-Transform方法" class="headerlink" title="Dataset Transform方法"></a>Dataset Transform方法</h2><p>Dataset transform方法提供了<a href="http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">concise syntax for chaining custom transformations</a></p>
<p>假如我们有两个方法</p>
<ol>
<li><code>withGreeting()</code>方法，该方法在原有的DataFrame基础上增加<code>greeting</code>列</li>
<li><code>withFarewell()</code>方法，该方法在原有的DataFrame基础上增加<code>farewell</code>列</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withGreeting</span></span>(df: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    df.withColumn(<span class="string">"greeting"</span>, lit(<span class="string">"hello world"</span>))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withFarewell</span></span>(df: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    df.withColumn(<span class="string">"farewell"</span>, lit(<span class="string">"goodbye"</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          <div class="read-more">
            <a href="/2019/01/19/Chaining-Custom-DataFrame-Transformations-in-Spark/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/01/10/共识算法概述/">共识算法概述</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-01-10
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <p>一开始就认为共识算法其实比较简单，但是一直没有认真地总结，导致这块知识一直都是短板。其实共识算法的本质是分布式一致性的问题，但是显然这和分布式一致性略有区别。有以下两个主要的区别：</p>
<ol>
<li>传统的分布式一致性算法认为，每个节点都不会欺诈，如果出现问题一般都是网络分区，请求顺序的问题。但是共识算法不能保证每一个节点都是正常节点，可能就是欺诈节点。</li>
<li>传统的一致性算法只要有值就行，不太关心哪个节点写入数据。<strong>但是共识算法，必须公平地对待每一个节点，选取这个节点必须是公平的，于是就有了POW和POS机制的出现。这个就好像是选择值本来一个很纯粹的事情绑定了特定的业务，所以要处理的事情就变得很复杂了。所以在分析共识算法的时候要将这个点剥离出来，拨出来之后这就是一个分布式一致性的问题。</strong></li>
</ol>
<p>所以说共识算法，剥离了上面两个方面的因素就是一个纯粹的分布式一致性算法。<strong>其实说白了共识算法就是分布式一致性在P2P网络中的应用。</strong></p>
<h2 id="从拜占庭将军问题谈起"><a href="#从拜占庭将军问题谈起" class="headerlink" title="从拜占庭将军问题谈起"></a>从拜占庭将军问题谈起</h2><p>这是一个很经典的分布式一致性问题的提出，这里我们就不做很多的说明。可以查看维基百科中关于这点的描述。</p>
<h2 id="拜占庭容错-BFT"><a href="#拜占庭容错-BFT" class="headerlink" title="拜占庭容错 BFT"></a>拜占庭容错 BFT</h2><p>这是拜占庭将军问题的早期的解决方案。在1982年的论文中提过几个解决方案。方案中把问题往下拆解，认为在“拜占庭将军”的问题可以在“军官与士官的问题”里解决，以降低将军问题的发生。而所谓的“军官与士官的问题”，就是探讨军官与他的士官是否能忠实实行命令。</p>
<p>其中一个解决方案认为即使出现了伪造或错误的消息。只要有问题的将军的数量不到三分之一，仍可以达到“拜占庭容错”。原因是把同样的标准下放到“军官与士官的问题”时，在背叛的军士官不足三分之一的情况下，有问题的军士官可以很容易的被纠出来。比如有军官A，士官B与士官C。当A要求B进攻，却要求C撤退时。只要B与C交换所收到的命令，就会立刻发现A有问题。以函数来表示，将军的总数为n，n里面背叛者的数量为t，则只要n &gt; 3t就可以容错。</p>
<p>另一个解决方案需要有无法消去的签名。在现今许多高度信息安全要求的关键系统里，数字签名就经常被用来实现拜占庭容错，找出有问题的将军。然而，在生命攸关系统里，使用 错误侦测码就可以大幅降低问题的发生。无论系统是否存在拜占庭将军问题。所以需要做密码运算的数字签名也不一定适合这类系统。</p>
<p>假如上述两个解决方案里，将军们无法直接通信时，该论文亦有进一步的解决方案。<br>此外，1980年代还有其他用来达到拜占庭容错的架构被提出，如：FTMP、MMFCS 与 SIFT。</p>
          <div class="read-more">
            <a href="/2019/01/10/共识算法概述/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/12/22/数据复制与一致性/">数据复制与一致性</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-12-22
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <p>在大数据存储系统中，为了增加系统高可用性，往往会将同一数据存储多份副本，工业界的常规做法是三备份。将数据复制成多份除了增加存储系统高可用性外还可以增加读操作的并发性，但是这样会引入一致性问题。本文会详细介绍。</p>
<h2 id="基本原则与设计理念"><a href="#基本原则与设计理念" class="headerlink" title="基本原则与设计理念"></a>基本原则与设计理念</h2><p>首先介绍一下CAP、BASE、ACID基础理论模型。</p>
<h3 id="CAP"><a href="#CAP" class="headerlink" title="CAP"></a>CAP</h3><ol>
<li><strong>强一致性</strong>：即在分布式系统中的同一数据多副本情形下，对于数据的更新操作体现出的效果与只有单份数据是一样的</li>
<li><strong>可用性</strong>：客户端在任何时刻对大规模数据系统的读/写操作都应该保证在限定延时内完成</li>
<li><strong>分区容忍性</strong>：在大规模分布式数据系统中，网络分布现象，即分区间的机器无法进行网络通信的情况是必然会发生的，所以系统应该能够在这种情况下仍然继续工作</li>
</ol>
<p>在设计具体分布式架构技术方案时，必须再一致性和可用性方面做出取舍，要么选择强一致性减弱服务可用性，要么选择高可用性容忍弱一致性。依然认为，传统的关系数据库在三要素中选择CA两个因素，即强一致性，高可用性，但是可扩展性与容错性差。而NoSQL系统往往更关注AP因素，即高扩展性和高可用性，但是往往以弱一致性作为代价。</p>
<h3 id="ACID"><a href="#ACID" class="headerlink" title="ACID"></a>ACID</h3><ol>
<li><strong>原子性(Atomicity)</strong></li>
<li><strong>一致性(Consistency)</strong></li>
<li><strong>事务独立(Isolation)</strong></li>
<li><strong>持久性(Durability)</strong></li>
</ol>
<h3 id="BASE"><a href="#BASE" class="headerlink" title="BASE"></a>BASE</h3><ol>
<li><strong>基本可用(Basically Available)</strong></li>
<li><strong>软状态或者柔性状态(Soft State)</strong></li>
<li><strong>最终一致性(Eventual Consistency)</strong></li>
</ol>
          <div class="read-more">
            <a href="/2018/12/22/数据复制与一致性/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/12/22/LeetCode-算法题目解答/">LeetCode 算法题目解答</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-12-22
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <p>为了督促自己能够每天刷一道算法题，特将LeetCode常见题目列出，之后每天一题，每完成一题都会总结解答并链接到该博客相应的题目上。</p>
          <div class="read-more">
            <a href="/2018/12/22/LeetCode-算法题目解答/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/12/21/Adaptive-Execution-让-Spark-SQL-更高效更智能/">Adaptive Execution 让 Spark SQL 更高效更智能</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-12-21
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>前面<a href="https://code-monkey.top/2018/12/13/Spark-SQL-Catalyst-%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86-%E4%B8%8E-RBO/">《Spark SQL / Catalyst 内部原理 与 RBO》</a>与<a href="https://code-monkey.top/2018/12/13/Spark-SQL-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%86%8D%E8%BF%9B%E4%B8%80%E6%AD%A5-CBO-%E5%9F%BA%E4%BA%8E%E4%BB%A3%E4%BB%B7%E7%9A%84%E4%BC%98%E5%8C%96/">《Spark SQL 性能优化再进一步 CBO 基于代价的优化》</a>介绍的优化，从查询本身与目标数据的特点的角度尽可能保证了最终生成的执行计划的高效性。但是</p>
<ul>
<li>执行计划一旦生成，便不可更改，即使执行过程中发现后续执行计划可以进一步优化，也只能按原计划执行</li>
<li>CBO 基于统计信息生成最优执行计划，需要提前生成统计信息，成本较大，且不适合数据更新频繁的场景</li>
<li>CBO 基于基础表的统计信息与操作对数据的影响推测中间结果的信息，只是估算，不够精确</li>
</ul>
<p>本文介绍的 Adaptive Execution 将可以根据执行过程中的中间数据优化后续执行，从而提高整体执行效率。核心在于两点</p>
<ul>
<li>执行计划可动态调整</li>
<li>调整的依据是中间结果的精确统计信息</li>
</ul>
          <div class="read-more">
            <a href="/2018/12/21/Adaptive-Execution-让-Spark-SQL-更高效更智能/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/12/13/Spark-SQL-性能优化再进一步-CBO-基于代价的优化/">Spark SQL 性能优化再进一步 CBO 基于代价的优化(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-12-13
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <h2 id="Spark-CBO-背景"><a href="#Spark-CBO-背景" class="headerlink" title="Spark CBO 背景"></a>Spark CBO 背景</h2><p>上文<a href="https://code-monkey.top/2018/12/13/Spark-SQL-Catalyst-%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86-%E4%B8%8E-RBO/">Spark SQL 内部原理中介绍的 Optimizer</a> 属于 RBO，实现简单有效。它属于 LogicalPlan 的优化，所有优化均基于 LogicalPlan 本身的特点，未考虑数据本身的特点，也未考虑算子本身的代价。</p>
<p>本文将介绍 CBO，它充分考虑了数据本身的特点（如大小、分布）以及操作算子的特点（中间结果集的分布及大小）及代价，从而更好的选择执行代价最小的物理执行计划，即 SparkPlan。</p>
<h2 id="Spark-CBO-原理"><a href="#Spark-CBO-原理" class="headerlink" title="Spark CBO 原理"></a>Spark CBO 原理</h2><p>CBO 原理是计算所有可能的物理计划的代价，并挑选出代价最小的物理执行计划。其核心在于评估一个给定的物理执行计划的代价。</p>
<p>物理执行计划是一个树状结构，其代价等于每个执行节点的代价总合，如下图所示。</p>
<img src="/2018/12/13/Spark-SQL-性能优化再进一步-CBO-基于代价的优化/spark_sql_cost_model.png">
<p>而每个执行节点的代价，分为两个部分</p>
<ul>
<li>该执行节点对数据集的影响，或者说该节点输出数据集的大小与分布</li>
<li>该执行节点操作算子的代价</li>
</ul>
<p>每个操作算子的代价相对固定，可用规则来描述。而执行节点输出数据集的大小与分布，分为两个部分：1) 初始数据集，也即原始表，其数据集的大小与分布可直接通过统计得到；2)中间节点输出数据集的大小与分布可由其输入数据集的信息与操作本身的特点推算。</p>
<p>所以，最终主要需要解决两个问题</p>
<ul>
<li>如何获取原始数据集的统计信息</li>
<li>如何根据输入数据集估算特定算子的输出数据集</li>
</ul>
          <div class="read-more">
            <a href="/2018/12/13/Spark-SQL-性能优化再进一步-CBO-基于代价的优化/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/12/13/Spark-SQL-Catalyst-内部原理-与-RBO/">Spark SQL / Catalyst 内部原理 与 RBO(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-12-13
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <h2 id="Spark-SQL-架构"><a href="#Spark-SQL-架构" class="headerlink" title="Spark SQL 架构"></a>Spark SQL 架构</h2><p>Spark SQL 的整体架构如下图所示</p>
<img src="/2018/12/13/Spark-SQL-Catalyst-内部原理-与-RBO/spark_sql.png">
<p>从上图可见，无论是直接使用 SQL 语句还是使用 DataFrame，都会经过如下步骤转换成 DAG 对 RDD 的操作</p>
<ul>
<li>Parser 解析 SQL，生成 Unresolved Logical Plan</li>
<li>由 Analyzer 结合 Catalog 信息生成 Resolved Logical Plan</li>
<li>Optimizer根据预先定义好的规则对 Resolved Logical Plan 进行优化并生成 Optimized Logical Plan</li>
<li>Query Planner 将 Optimized Logical Plan 转换成多个 Physical Plan</li>
<li>CBO 根据 Cost Model 算出每个 Physical Plan 的代价并选取代价最小的 Physical Plan 作为最终的 Physical Plan</li>
<li>Spark 以 DAG 的方法执行上述 Physical Plan</li>
<li>在执行 DAG 的过程中，Adaptive Execution 根据运行时信息动态调整执行计划从而提高执行效率</li>
</ul>
          <div class="read-more">
            <a href="/2018/12/13/Spark-SQL-Catalyst-内部原理-与-RBO/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/12/11/Spark-CommitCoordinator-保证数据一致性/">Spark CommitCoordinator 保证数据一致性(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-12-11
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark 输出数据到 HDFS 时，需要解决如下问题：</p>
<ol>
<li>由于多个 Task 同时写数据到 HDFS，如何保证要么所有 Task 写的所有文件要么同时对外可见，要么同时对外不可见，即保证数据一致性</li>
<li>同一 Task 可能因为 Speculation 而存在两个完全相同的 Task 实例写相同的数据到 HDFS中，如何保证只有一个 commit 成功</li>
<li>对于大 Job（如具有几万甚至几十万 Task），如何高效管理所有文件</li>
</ol>
<h2 id="commit-原理"><a href="#commit-原理" class="headerlink" title="commit 原理"></a>commit 原理</h2><p>本文通过 Local mode 执行如下 Spark 程序详解 commit 原理</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sparkContext.textFile(<span class="string">"/jason/input.zstd"</span>)</span><br><span class="line">  .map(_.split(<span class="string">","</span>))</span><br><span class="line">  .saveAsTextFile(<span class="string">"/jason/test/tmp"</span>)</span><br></pre></td></tr></table></figure>
<p>在详述 commit 原理前，需要说明几个述语</p>
<ul>
<li>Task，即某个 Application 的某个 Job 内的某个 Stage 的一个 Task</li>
<li>TaskAttempt，Task 每次执行都视为一个 TaskAttempt。对于同一个 Task，可能同时存在多个 TaskAttemp</li>
<li>Application Attempt，即 Application 的一次执行</li>
</ul>
<p>在本文中，会使用如下缩写</p>
<ul>
<li>${output.dir.root} 即输出目录根路径</li>
<li>${appAttempt} 即 Application Attempt ID，为整型，从 0 开始</li>
<li>${taskAttemp} 即 Task Attetmp ID，为整型，从 0 开始</li>
</ul>
          <div class="read-more">
            <a href="/2018/12/11/Spark-CommitCoordinator-保证数据一致性/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
      
  <nav class="pagination">
    
    
      <a class="next" href="/page/2/">
        <span class="next-text">下一页</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>


    
  </section>

          </div>
          

        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:tanghuaidong@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
        
          <a href="https://github.com/tangboy" class="iconfont icon-github" title="github"></a>
        
      
    
      
    
      
        
          <a href="https://www.zhihu.com/people/tang-huai-dong/activities" class="iconfont icon-zhihu" title="zhihu"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    
      
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>



<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2015 - 
    
    2019

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Anthon</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    


    
  



  
  





  
    <script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

  
    <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=2.10.1"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  </body>
</html>
