<!DOCTYPE html>
<html lang="zh-cn">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="机器学习，编程，玩">













  <link rel="alternate" href="/default" title="Anthon">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.10.1">



<link rel="canonical" href="http://code-monkey.top/page/2/">



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css">




  <link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css">



<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.10.1">



  



  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>









<script>
  window.config = {"leancloud":{"app_id":null,"app_key":null},"toc":true,"fancybox":true,"pjax":true};
</script>

    <title> Anthon </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/." class="logo">Anthon</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            标签
          
        </li>
      </a>
    
      <a href="/about">
        <li class="mobile-menu-item">
          
          
            关于
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Anthon</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/about">
            
            
              关于
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <section id="posts" class="posts">
    
      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/04/24/Spark-Streaming状态管理应用优化之路/">Spark-Streaming状态管理应用优化之路(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-04-24
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <p>通常来说，使用Spark-Streaming做无状态的流式计算是很方便的，每个batch时间间隔内仅需要计算当前时间间隔的数据即可，不需要关注之前的状态。但是很多时候，我们需要对一些数据做跨周期的统计，例如我们需要统计一个小时内每个用户的行为，我们定义的计算间隔(batch-duration)肯定会比一个小时小，一般是数十秒到几分钟左右，每个batch的计算都要更新最近一小时的用户行为，所以需要在整个计算过程中维护一个状态来保存近一个小时的用户行为。在Spark-1.6以前，可以通过updateStateByKey操作实现有状态的流式计算，从spark-1.6开始，新增了mapWithState操作，引入了一种新的流式状态管理机制。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>为了更形象的介绍Spark-Streaming中的状态管理，我们从一个简单的问题展开：我们需要实时统计近一小时内每个用户的行为(点击、购买等)，为了简单，就把这个行为看成点击列表吧，来一条记录，则加到指定用户的点击列表中，并保证点击列表无重复。计算时间间隔为1分钟，即每1分钟更新近一小时用户行为，并将有状态变化的用户行为输出。</p>
          <div class="read-more">
            <a href="/2019/04/24/Spark-Streaming状态管理应用优化之路/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/04/22/Spark-Scheduler内部原理剖析/">Spark Scheduler内部原理剖析(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-04-22
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <p>通过文章<a href="https://code-monkey.top/2019/04/22/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5RDD/">“Spark核心概念RDD”</a>我们知道，Spark的核心是根据RDD来实现的，Spark Scheduler则为Spark核心实现的重要一环，其作用就是任务调度。Spark的任务调度就是如何组织任务去处理RDD中每个分区的数据，根据RDD的依赖关系构建DAG，基于DAG划分Stage，将每个Stage中的任务发到指定节点运行。基于Spark的任务调度原理，我们可以合理规划资源利用，做到尽可能用最少的资源高效地完成任务计算。</p>
<h2 id="分布式运行框架"><a href="#分布式运行框架" class="headerlink" title="分布式运行框架"></a>分布式运行框架</h2><p>Spark可以部署在多种资源管理平台，例如Yarn、Mesos等，Spark本身也实现了一个简易的资源管理机制，称之为Standalone模式。由于工作中接触较多的是Saprk on Yarn，不做特别说明，以下所述均表示Spark-on-Yarn。Spark部署在Yarn上有两种运行模式，分别为yarn-client和yarn-cluster模式，它们的区别仅仅在于Spark Driver是运行在Client端还是ApplicationMaster端。如下图所示为Spark部署在Yarn上，以yarn-cluster模式运行的分布式计算框架。</p>
<img src="/2019/04/22/Spark-Scheduler内部原理剖析/spark-distribution-framework.png">
<p>其中蓝色部分是Spark里的概念，包括Client、ApplicationMaster、Driver和Executor，其中Client和ApplicationMaster主要是负责与Yarn进行交互；Driver作为Spark应用程序的总控，负责分发任务以及监控任务运行状态；Executor负责执行任务，并上报状态信息给Driver，从逻辑上来看Executor是进程，运行在其中的任务是线程，所以说Spark的任务是线程级别的。通过下面的时序图可以更清晰地理解一个Spark应用程序从提交到运行的完整流程。</p>
<img src="/2019/04/22/Spark-Scheduler内部原理剖析/spark-submit-time.png">
<p>提交一个Spark应用程序，首先通过Client向ResourceManager请求启动一个Application，同时检查是否有足够的资源满足Application的需求，如果资源条件满足，则准备ApplicationMaster的启动上下文，交给ResourceManager，并循环监控Application状态。</p>
<p>当提交的资源队列中有资源时，ResourceManager会在某个NodeManager上启动ApplicationMaster进程，ApplicationMaster会单独启动Driver后台线程，当Driver启动后，ApplicationMaster会通过本地的RPC连接Driver，并开始向ResourceManager申请Container资源运行Executor进程（一个Executor对应与一个Container），当ResourceManager返回Container资源，则在对应的Container上启动Executor。</p>
<p>Driver线程主要是初始化SparkContext对象，准备运行所需的上下文，然后一方面保持与ApplicationMaster的RPC连接，通过ApplicationMaster申请资源，另一方面根据用户业务逻辑开始调度任务，将任务下发到已有的空闲Executor上。</p>
<p>当ResourceManager向ApplicationMaster返回Container资源时，ApplicationMaster就尝试在对应的Container上启动Executor进程，Executor进程起来后，会向Driver注册，注册成功后保持与Driver的心跳，同时等待Driver分发任务，当分发的任务执行完毕后，将任务状态上报给Driver。</p>
<blockquote>
<blockquote>
<p>Driver把资源申请的逻辑给抽象出来，以适配不同的资源管理系统，所以才间接地通过ApplicationMaster去和Yarn打交道。</p>
</blockquote>
</blockquote>
<p>从上述时序图可知，Client只管提交Application并监控Application的状态。对于Spark的任务调度主要是集中在两个方面: 资源申请和任务分发，其主要是通过ApplicationMaster、Driver以及Executor之间来完成，下面详细剖析Spark任务调度每个细节。</p>
          <div class="read-more">
            <a href="/2019/04/22/Spark-Scheduler内部原理剖析/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/04/22/Spark核心概念RDD/">Spark核心概念RDD(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-04-22
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <p>RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)，它是一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等)，通过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，该DAG描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)一气呵成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记录被传入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建迭代型应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系统，简单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。</p>
<h2 id="RDD特点"><a href="#RDD特点" class="headerlink" title="RDD特点"></a>RDD特点</h2><p>RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p>
<h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>如下图所示，RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。</p>
<img src="/2019/04/22/Spark核心概念RDD/rdd-partition.png">
          <div class="read-more">
            <a href="/2019/04/22/Spark核心概念RDD/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/04/18/PySpark-的背后原理/">PySpark 的背后原理(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-04-18
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <p>Spark主要是由Scala语言开发，为了方便和其他系统集成而不引入scala相关依赖，部分实现使用Java语言开发，例如External Shuffle Service等。总体来说，Spark是由JVM语言实现，会运行在JVM中。然而，Spark除了提供Scala/Java开发接口外，还提供了Python、R等语言的开发接口，为了保证Spark核心实现的独立性，Spark仅在外围做包装，实现对不同语言的开发支持，本文主要介绍Python Spark的实现原理，剖析pyspark应用程序是如何运行起来的。</p>
<h2 id="Spark运行时架构"><a href="#Spark运行时架构" class="headerlink" title="Spark运行时架构"></a>Spark运行时架构</h2><p>首先我们先回顾下Spark的基本运行时架构，如下图所示，其中橙色部分表示为JVM，Spark应用程序运行时主要分为Driver和Executor，Driver负载总体调度及UI展示，Executor负责Task运行，Spark可以部署在多种资源管理系统中，例如Yarn、Mesos等，同时Spark自身也实现了一种简单的Standalone(独立部署)资源管理系统，可以不用借助其他资源管理系统即可运行。更多细节请参考<a href="https://code-monkey.top/2019/04/22/Spark-Scheduler%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90/">Spark Scheduler内部原理剖析</a>。</p>
<img src="/2019/04/18/PySpark-的背后原理/spark-structure.png">
<p>用户的Spark应用程序运行在Driver上(某种程度上说，用户的程序就是Spark Driver程序)，经过Spark调度封装成一个个Task，再将这些Task信息发给Executor执行，Task信息包括代码逻辑以及数据信息，Executor不直接运行用户的代码。</p>
          <div class="read-more">
            <a href="/2019/04/18/PySpark-的背后原理/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/04/09/Kafka深度解析/">Kafka深度解析(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-04-09
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><h3 id="Kafka简介"><a href="#Kafka简介" class="headerlink" title="Kafka简介"></a>Kafka简介</h3><p>Kafka是一种分布式的，基于发布/订阅的消息系统。主要设计目标如下：</p>
<ul>
<li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能</li>
<li>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输</li>
<li>支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输</li>
<li>同时支持离线数据处理和实时数据处理</li>
</ul>
<h3 id="为什么要用消息系统"><a href="#为什么要用消息系统" class="headerlink" title="为什么要用消息系统"></a>为什么要用消息系统</h3><ul>
<li><p><strong>解耦</strong><br>在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息队列在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束</p>
</li>
<li><p><strong>冗余</strong><br>有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。在被许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理过程明确的指出该消息已经被处理完毕，确保你的数据被安全的保存直到你使用完毕。</p>
</li>
<li><p><strong>扩展性</strong><br>因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的；只要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。</p>
</li>
<li><p><strong>灵活性 &amp; 峰值处理能力</strong><br>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃</p>
</li>
<li><p><strong>可恢复性</strong><br>当体系的一部分组件失效，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。而这种允许重试或者延后处理请求的能力通常是造就一个略感不便的用户和一个沮丧透顶的用户之间的区别。</p>
</li>
<li><p><strong>送达保证</strong><br>消息队列提供的冗余机制保证了消息能被实际的处理，只要一个进程读取了该队列即可。在此基础上，部分消息系统提供了一个”只送达一次”保证。无论有多少进程在从队列中领取数据，每一个消息只能被处理一次。这之所以成为可能，是因为获取一个消息只是”预定”了这个消息，暂时把它移出了队列。除非客户端明确的表示已经处理完了这个消息，否则这个消息会被放回队列中去，在一段可配置的时间之后可再次被处理。</p>
</li>
<li><p><strong>顺序保证</strong><br>在大多使用场景下，数据处理的顺序都很重要。消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。部分消息系统保证消息通过FIFO（先进先出）的顺序来处理，因此消息在队列中的位置就是从队列中检索他们的位置。</p>
</li>
<li><p><strong>缓冲</strong><br>在任何重要的系统中，都会有需要不同的处理时间的元素。例如,加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行–写入队列的处理会尽可能的快速，而不受从队列读的预备处理的约束。该缓冲有助于控制和优化数据流经过系统的速度。</p>
</li>
<li><p><strong>理解数据流</strong><br>在一个分布式系统里，要得到一个关于用户操作会用多长时间及其原因的总体印象，是个巨大的挑战。消息队列通过消息被处理的频率，来方便的辅助确定那些表现不佳的处理过程或领域，这些地方的数据流都不够优化。</p>
</li>
<li><p><strong>异步通信</strong><br>很多时候，你不想也不需要立即处理消息。消息队列提供了异步处理机制，允许你把一个消息放入队列，但并不立即处理它。你想向队列中放入多少消息就放多少，然后在你乐意的时候再去处理它们。</p>
</li>
</ul>
          <div class="read-more">
            <a href="/2019/04/09/Kafka深度解析/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/04/01/美团DB数据同步到数据仓库的架构与实践/">美团DB数据同步到数据仓库的架构与实践(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-04-01
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在数据仓库建模中，未经任何加工处理的原始业务层数据，我们称之为ODS(Operational Data Store)数据。在互联网企业中，常见的ODS数据有业务日志数据（Log）和业务DB数据（DB）两类。对于业务DB数据来说，从MySQL等关系型数据库的业务数据进行采集，然后导入到Hive中，是进行数据仓库生产的重要环节。</p>
<p>如何准确、高效地把MySQL数据同步到Hive中？一般常用的解决方案是批量取数并Load：直连MySQL去Select表中的数据，然后存到本地文件作为中间存储，最后把文件Load到Hive表中。这种方案的优点是实现简单，但是随着业务的发展，缺点也逐渐暴露出来：</p>
<ul>
<li>性能瓶颈：随着业务规模的增长，Select From MySQL -&gt; Save to Localfile -&gt; Load to Hive这种数据流花费的时间越来越长，无法满足下游数仓生产的时间要求。</li>
<li>直接从MySQL中Select大量数据，对MySQL的影响非常大，容易造成慢查询，影响业务线上的正常服务。</li>
<li>由于Hive本身的语法不支持更新、删除等SQL原语，对于MySQL中发生Update/Delete的数据无法很好地进行支持。</li>
</ul>
<p>为了彻底解决这些问题，我们逐步转向CDC (Change Data Capture) + Merge的技术方案，即实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。Binlog是MySQL的二进制日志，记录了MySQL中发生的所有数据变更，MySQL集群自身的主从同步就是基于Binlog做的。</p>
<p>本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入数仓。</p>
<h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><img src="/2019/04/01/美团DB数据同步到数据仓库的架构与实践/1.png">
          <div class="read-more">
            <a href="/2019/04/01/美团DB数据同步到数据仓库的架构与实践/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/02/28/知乎如何基于开源Druid打造下一代数据平台/">知乎如何基于开源Druid打造下一代数据平台(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-02-28
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <img src="/2019/02/28/知乎如何基于开源Druid打造下一代数据平台/1.webp">
<h2 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h2><p>本文重点介绍了知乎数据分析平台对 Druid 的查询优化。通过自研的一整套缓存机制和查询改造，该平台目前在较长的时间内，满足了业务固化的指标需求和灵活的分析需求，减少了数据开发者的开发成本。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>知乎作为知名中文知识内容平台，业务增长和产品迭代速度很快，如何满足业务快速扩张中的灵活分析需求，是知乎数据平台组要面临的一大挑战。</p>
<p>知乎数据平台团队基于开源的 Druid 打造的业务自助式的数据分析平台，经过研发迭代，目前支撑了全业务的数据分析需求，是业务数据分析的重要工具。</p>
<p>目前，平台主要的能力如下：</p>
<ol>
<li>统一的数据源管理，支持摄入离线数仓的 Hive 表和实时数仓的 Kafka 流</li>
<li>自助式报表配置，支持多维分析报表、留存分析报表</li>
<li>灵活的多维度多指标的组合分析，秒级响应速度，支持嵌套式「与」和「或」条件筛选</li>
<li>自助式仪表盘配置</li>
<li>开发平台接口，为其他系统提供数据服务</li>
<li>统一的数据权限管理</li>
</ol>
<p>目前，业务使用平台的数据如下：</p>
<ol>
<li>自助式配置仪表盘数：495 个，仪表盘内报表共计：2399 张</li>
<li>日请求量 3w+</li>
<li>为 A/B Testing、渠道管理、APM 、数据邮件等系统提供数据 API</li>
</ol>
<h2 id="数据分析平台架构"><a href="#数据分析平台架构" class="headerlink" title="数据分析平台架构"></a>数据分析平台架构</h2><img src="/2019/02/28/知乎如何基于开源Druid打造下一代数据平台/2.webp">
          <div class="read-more">
            <a href="/2019/02/28/知乎如何基于开源Druid打造下一代数据平台/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/02/28/埋点数据/">埋点数据(转)</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-02-28
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <img src="/2019/02/28/埋点数据/1.webp">
<h2 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h2><p>埋点作为商业智能（BI）和人工智能（AI）体系中重要的一环，是公司提升产品工程质量、实施 AB Testing、个性化推荐服务重要的数据来源。在传统的纯 Web 和 Native 开发的产品中，埋点从技术的角度来说未必多深奥，但从业务的角度来说要做到埋点设计规范、流程高效和保证质量却是很难。本文重点介绍一下知乎客户端的埋点模型、流程和平台技术。</p>
<h2 id="客户端埋点为什么难？"><a href="#客户端埋点为什么难？" class="headerlink" title="客户端埋点为什么难？"></a>客户端埋点为什么难？</h2><p>Web 端的埋点可以随着新代码上线即时生效，对版本的发车概念相对较弱，即使埋点错漏，修复成本较低。</p>
<p>对客户端而言，如果使用 Native 技术开发的功能埋点有问题，则需要等下一个版本才能修复，并且还有版本覆盖度的问题。修复埋点的这个时间窗口一般都比较长，会对业务的产品快速迭代产生很负面的影响。从业务的角度来说，客户端在发布功能之前，对于要做的数据分析不见得想得全，无计划收集非常多的埋点，对于埋点设计人员、客户端开发、测试人员来说是很大的工作量。反过来说，真正要用数时才发现重要的埋点没有采集，则会 「点」 到用时方恨少。因此，如何综合规划一个版本要采集的埋点，也是颇有挑战的事情，颇有「养兵千日，用兵一时」的感觉。</p>
          <div class="read-more">
            <a href="/2019/02/28/埋点数据/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/01/19/Chaining-Custom-PySpark-DataFrame-Transformations/">Chaining Custom PySpark DataFrame Transformations</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-01-19
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <p>和基于Scala Spark API一样，我们也期望在Pyspark中也能够将不同的transformation方法连接起来统一执行。</p>
<p>这篇博客主要描述如何给Pyspark DataFrame添加<code>transform</code>方法，从而支持能够将自定义的DataFrame transformation连接起来。</p>
<p>同时，我们也会介绍如何利用<a href="https://github.com/pytoolz/cytoolz" target="_blank" rel="noopener">cytoolz</a>来批量顺序执行自定义变换函数</p>
<h2 id="Chaining-DataFrame-Transformations-with-Lambda"><a href="#Chaining-DataFrame-Transformations-with-Lambda" class="headerlink" title="Chaining DataFrame Transformations with Lambda"></a>Chaining DataFrame Transformations with Lambda</h2><p>首先，在原生的Pyspark DataFrame增加<code>transform</code>方法，从而我们能够串联执行DataFrame变换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.dataframe <span class="keyword">import</span> DataFrame</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, f)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> f(self)</span><br><span class="line">    </span><br><span class="line">DataFrame.transform = transform</span><br></pre></td></tr></table></figure>
<p>接下来，我们定义一些简单的DataFrame变换方法来测试<code>transform</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">with_greeting</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> df.withColumn(<span class="string">"greeting"</span>, lit(<span class="string">"hi"</span>))</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">with_something</span><span class="params">(df, something)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> df.withColumn(<span class="string">"something"</span>, lit(something))</span><br></pre></td></tr></table></figure>
<p>创建一个DataFrame然后串联执行<code>with_greeting</code>和<code>with_something</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">data = [(<span class="string">"jsoe"</span>, <span class="number">1</span>), (<span class="string">"li"</span>, <span class="number">2</span>), (<span class="string">"liz"</span>, <span class="number">3</span>)]</span><br><span class="line">source_df = spark.createDataFrame(data, [<span class="string">"name"</span>, <span class="string">"age"</span>])</span><br><span class="line"></span><br><span class="line">actual_df = (source_df</span><br><span class="line">                .transform(<span class="keyword">lambda</span> df: with_greeting(df))</span><br><span class="line">                .transform(<span class="keyword">lambda</span> df: with_something(df, <span class="string">"crazy"</span>)))</span><br><span class="line">                </span><br><span class="line">print(actual_df.show())</span><br><span class="line"></span><br><span class="line">+----+---+--------+---------+</span><br><span class="line">|name|age|greeting|something|</span><br><span class="line">+----+---+--------+---------+</span><br><span class="line">|jose|  <span class="number">1</span>|      hi|    crazy|</span><br><span class="line">|  li|  <span class="number">2</span>|      hi|    crazy|</span><br><span class="line">| liz|  <span class="number">3</span>|      hi|    crazy|</span><br><span class="line">+----+---+--------+---------+</span><br></pre></td></tr></table></figure>
<p>对于只有一个DataFrame参数的自定义变换中，<code>lambda</code>是可以省略的，从而我们可以简化调用方式如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">actual_df = (source_df</span><br><span class="line">             .transform(with_greeting)</span><br><span class="line">             .transform(<span class="keyword">lambda</span> df: with_something(df, <span class="string">"crazy"</span>)))</span><br></pre></td></tr></table></figure>
<p>如果我们没有定义<code>DataFrame#transform</code>方法，我们就不需要像下面的代码一样，来调用不同的transformation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1 = with_greeting(source_df)</span><br><span class="line">actual_df = with_something(df1, <span class="string">"moo"</span>)</span><br></pre></td></tr></table></figure>
<p>比较上述代码，采用<code>transform</code>来调用不同的DataFrame的变化，能够避免定义中间DataFrame，从而使得代码更加清晰</p>
<p>接下来，我们进一步探讨transformations的其他定义形式，让整个<code>transform</code>更加清晰明了</p>
          <div class="read-more">
            <a href="/2019/01/19/Chaining-Custom-PySpark-DataFrame-Transformations/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2019/01/19/Chaining-Custom-DataFrame-Transformations-in-Spark/">Chaining Custom DataFrame Transformations in Spark</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-01-19
        </span>
        
        
      </div>
    </header>

    
    


    <div class="post-content">
      
        
        
          
        

        
          <p>在Spark中，可以采用<code>implicit classes</code>或者<code>Dataset#transform</code>来连接DataFrame的变换，这篇博客着重描如何连接DataFrame变换操作，并且详细解释说明为啥<code>Dataset#transform</code>方式要比<code>implicit classes</code>更有优势。</p>
<h2 id="Dataset-Transform方法"><a href="#Dataset-Transform方法" class="headerlink" title="Dataset Transform方法"></a>Dataset Transform方法</h2><p>Dataset transform方法提供了<a href="http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">concise syntax for chaining custom transformations</a></p>
<p>假如我们有两个方法</p>
<ol>
<li><code>withGreeting()</code>方法，该方法在原有的DataFrame基础上增加<code>greeting</code>列</li>
<li><code>withFarewell()</code>方法，该方法在原有的DataFrame基础上增加<code>farewell</code>列</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withGreeting</span></span>(df: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    df.withColumn(<span class="string">"greeting"</span>, lit(<span class="string">"hello world"</span>))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withFarewell</span></span>(df: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    df.withColumn(<span class="string">"farewell"</span>, lit(<span class="string">"goodbye"</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          <div class="read-more">
            <a href="/2019/01/19/Chaining-Custom-DataFrame-Transformations-in-Spark/" class="read-more-link">阅读更多</a>
          </div>
        
      
    </div>

    

    

  </article>

      
      
  <nav class="pagination">
    
      <a class="prev" href="/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text">上一页</span>
      </a>
    
    
      <a class="next" href="/page/3/">
        <span class="next-text">下一页</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>


    
  </section>

          </div>
          

        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:tanghuaidong@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
        
          <a href="https://github.com/tangboy" class="iconfont icon-github" title="github"></a>
        
      
    
      
    
      
        
          <a href="https://www.zhihu.com/people/tang-huai-dong/activities" class="iconfont icon-zhihu" title="zhihu"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    
      
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>



<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2015 - 
    
    2019

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Anthon</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    


    
  



  
  





  
    <script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

  
    <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=2.10.1"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

  </body>
</html>
