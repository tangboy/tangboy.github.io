<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="聚类算法"/>




  <meta name="keywords" content="python, 聚类算法, kmeans, Anthon" />










  <link rel="alternate" href="/default" title="Anthon">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.10.1" />



<link rel="canonical" href="http://code-monkey.top/2017/10/14/聚类算法/"/>



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css" />




  <link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css" />



<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.10.1" />



  



  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "MMvtgrWtn9BuiNVh6B529AP5-gzGzoHsz",
      appKey: "bA46NPkYiSG0QaBP2vX2ckzl"
    });
  </script>





<script>
  window.config = {"leancloud":{"app_id":"MMvtgrWtn9BuiNVh6B529AP5-gzGzoHsz","app_key":"bA46NPkYiSG0QaBP2vX2ckzl"},"toc":true,"fancybox":true,"pjax":true};
</script>

    <title> 聚类算法 - Anthon </title>
  <meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/." class="logo">Anthon</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            标签
          
        </li>
      </a>
    
      <a href="/about">
        <li class="mobile-menu-item">
          
          
            关于
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Anthon</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/about">
            
            
              关于
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          聚类算法
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2017-10-14
        </span>
        
        
        <span class="post-visits"
             data-url="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"
             data-title="聚类算法">
          阅读次数 0
        </span>
        
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#K-Means"><span class="toc-text">K-Means</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#简介"><span class="toc-text">简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法大致流程"><span class="toc-text">算法大致流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#完整计算过程"><span class="toc-text">完整计算过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means的不足"><span class="toc-text">K-Means的不足</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means-算法"><span class="toc-text">K-Means++算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sklearn包中的K-Means算法"><span class="toc-text">sklearn包中的K-Means算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AP算法"><span class="toc-text">AP算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#算法简介"><span class="toc-text">算法简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#相关概念-假如有数据点i和数据点j"><span class="toc-text">相关概念(假如有数据点i和数据点j)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数学公式"><span class="toc-text">数学公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#详细的算法流程"><span class="toc-text">详细的算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sklearn包中的AP算法"><span class="toc-text">sklearn包中的AP算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AP算法的优点"><span class="toc-text">AP算法的优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AP算法的不足"><span class="toc-text">AP算法的不足</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mean-shift"><span class="toc-text">Mean-shift</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#概述"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#图解过程"><span class="toc-text">图解过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mean-shift-算法函数"><span class="toc-text">Mean-shift 算法函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spectral-Clustering"><span class="toc-text">Spectral Clustering</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#概述-1"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#图解过程-1"><span class="toc-text">图解过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spectral-Clustering算法函数"><span class="toc-text">Spectral Clustering算法函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#核心函数：sklearn-cluster-SpectralClustering"><span class="toc-text">核心函数：sklearn.cluster.SpectralClustering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#主要参数-参数较多，详细参数"><span class="toc-text">主要参数(参数较多，详细参数)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#主要属性"><span class="toc-text">主要属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法示例：代码中有详细讲解内容"><span class="toc-text">算法示例：代码中有详细讲解内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#效果图"><span class="toc-text">效果图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hierarchical-Clustering"><span class="toc-text">Hierarchical Clustering</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#概述-2"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法步骤"><span class="toc-text">算法步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#图解过程-2"><span class="toc-text">图解过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hierarchical-Clustering算法函数"><span class="toc-text">Hierarchical Clustering算法函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DBSCAN"><span class="toc-text">DBSCAN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#概述-3"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法步骤（大致非详细）"><span class="toc-text">算法步骤（大致非详细）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#图解过程-3"><span class="toc-text">图解过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DBSCAN算法函数"><span class="toc-text">DBSCAN算法函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#效果图-1"><span class="toc-text">效果图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法优缺点"><span class="toc-text">算法优缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Birch"><span class="toc-text">Birch</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#概述-4"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#相关概念："><span class="toc-text">相关概念：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#图解过程-4"><span class="toc-text">图解过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Birch算法函数"><span class="toc-text">Birch算法函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法示例：代码中有详细讲解内容-1"><span class="toc-text">算法示例：代码中有详细讲解内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#效果图：分别为n-clusters-None-和n-clusters-4"><span class="toc-text">效果图：分别为n_clusters &#x3D; None 和n_clusters &#x3D; 4</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GaussianMixtureModel-补"><span class="toc-text">GaussianMixtureModel(补)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#概述-5"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数学公式-1"><span class="toc-text">数学公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GaussianMixtureModel-算法函数"><span class="toc-text">GaussianMixtureModel 算法函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法示例：代码中有详细讲解内容-2"><span class="toc-text">算法示例：代码中有详细讲解内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#效果图-2"><span class="toc-text">效果图</span></a></li></ol></li></ol>
    </div>
  </div>



    <div class="post-content">
      
        <h2 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>K-means算法是最为经典的基于划分的聚类方法，是十大经典数据挖掘算法之一。K-means算法的基本思想是：以空间中k个点为中心进行聚类，对最靠近他们的对象归类。通过迭代的方法，逐次更新各聚类中心的值，直至得到最好的聚类结果。</p>
<h3 id="算法大致流程"><a href="#算法大致流程" class="headerlink" title="算法大致流程"></a>算法大致流程</h3><ol>
<li>随机选取k个点作为种子点(这k个点不一定属于数据集)</li>
<li>分别计算每个数据点到k个种子点的距离，离哪个种子点最近，就属于哪类</li>
<li>重新计算k个种子点的坐标(简单常用的方法是求坐标值的平均值作为新的坐标值)</li>
<li>重复2、3步，直到种子点坐标不变或者循环次数完成</li>
</ol>
<h3 id="完整计算过程"><a href="#完整计算过程" class="headerlink" title="完整计算过程"></a>完整计算过程</h3><ol>
<li><p>设置实验数据<br>运行之后，效果如下图所示：</p>
<img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_1.png" class="">
<p>在图中，ABCDE五个点是待分类点，k1、k2是两个种子点。</p>
</li>
<li><p>计算ABCDE五个点到k1、k2的距离，离哪个点近，就属于哪个点，进行初步分类。<br>  结果如图：</p>
  <img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_2.png" class="">
<p>   A、B属于k1，C、D、E属于k2</p>
</li>
<li><p>重新计算k1、k2的坐标。这里使用简单的坐标的平均值，使用其他算法也可以(例如以下三个公式)<br> a) Minkowski Distance公式——λ可以随意取值，可以是负数，也可以是正数，或是无穷大。<br> b) Euclidean Distance公式——也就是第一个公式λ=2的情况<br> c) CityBlock Distance公式——也就是第一个公式λ=1的情况</p>
<p> 采用坐标平均值算法的结果如图：</p>
  <img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_3.png" class="">
</li>
<li><p>重复2、3步，直到最终分类完毕。下面是完整的示例代码：</p>
</li>
</ol>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">##样本数据(Xi,Yi)，需要转换成数组(列表)形式</span></span><br><span class="line">Xn=np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">1.9</span>,<span class="number">2.5</span>,<span class="number">4</span>])</span><br><span class="line">Yn=np.array([<span class="number">5</span>,<span class="number">4.8</span>,<span class="number">4</span>,<span class="number">1.8</span>,<span class="number">2.2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#标识符号</span></span><br><span class="line">sign_n = [<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>,<span class="string">'E'</span>]</span><br><span class="line">sign_k = [<span class="string">'k1'</span>,<span class="string">'k2'</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_class</span><span class="params">(Xk,Yk)</span>:</span></span><br><span class="line">    <span class="comment">##数据点分类</span></span><br><span class="line">    cls_dict = &#123;&#125;</span><br><span class="line">    <span class="comment">##离哪个分类点最近，属于哪个分类</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xn)):</span><br><span class="line">        temp = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(Xk)):</span><br><span class="line">            d1 = np.sqrt((Xn[i]-Xk[j])*(Xn[i]-Xk[j])+(Yn[i]-Yk[j])*(Yn[i]-Yk[j]))</span><br><span class="line">            temp.append(d1)</span><br><span class="line">        min_dis=np.min(temp)</span><br><span class="line">        min_inx = temp.index(min_dis)</span><br><span class="line">        cls_dict[sign_n[i]]=sign_k[min_inx]</span><br><span class="line">    <span class="comment">#print(cls_dict)</span></span><br><span class="line">    <span class="keyword">return</span> cls_dict</span><br><span class="line">    </span><br><span class="line"><span class="comment">##重新计算分类的坐标点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recal_class_point</span><span class="params">(Xk,Yk,cls_dict)</span>:</span>  </span><br><span class="line">    num_k1 = <span class="number">0</span>  <span class="comment">#属于k1的数据点的个数</span></span><br><span class="line">    num_k2 = <span class="number">0</span>  <span class="comment">#属于k2的数据点的个数</span></span><br><span class="line">    x1 =<span class="number">0</span>       <span class="comment">#属于k1的x坐标和</span></span><br><span class="line">    y1 =<span class="number">0</span>       <span class="comment">#属于k1的y坐标和</span></span><br><span class="line">    x2 =<span class="number">0</span>       <span class="comment">#属于k2的x坐标和</span></span><br><span class="line">    y2 =<span class="number">0</span>       <span class="comment">#属于k2的y坐标和</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">##循环读取已经分类的数据</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> cls_dict:</span><br><span class="line">        <span class="comment">##读取d的类别</span></span><br><span class="line">        kk = cls_dict[d]</span><br><span class="line">        <span class="keyword">if</span> kk == <span class="string">'k1'</span>:</span><br><span class="line">            <span class="comment">#读取d在数据集中的索引</span></span><br><span class="line">            idx = sign_n.index(d)</span><br><span class="line">            <span class="comment">##累加x值</span></span><br><span class="line">            x1 += Xn[idx]</span><br><span class="line">            <span class="comment">##累加y值</span></span><br><span class="line">            y1 += Yn[idx]</span><br><span class="line">            <span class="comment">##累加分类个数</span></span><br><span class="line">            num_k1 += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="comment">#读取d在数据集中的索引</span></span><br><span class="line">            idx = sign_n.index(d)</span><br><span class="line">            <span class="comment">##累加x值</span></span><br><span class="line">            x2 += Xn[idx]</span><br><span class="line">            <span class="comment">##累加y值</span></span><br><span class="line">            y2 += Yn[idx]</span><br><span class="line">            <span class="comment">##累加分类个数</span></span><br><span class="line">            num_k2 += <span class="number">1</span></span><br><span class="line">    <span class="comment">##求平均值获取新的分类坐标点</span></span><br><span class="line">    k1_new_x = x1/num_k1 <span class="comment">#新的k1的x坐标</span></span><br><span class="line">    k1_new_y = y1/num_k1 <span class="comment">#新的k1的y坐标</span></span><br><span class="line"></span><br><span class="line">    k2_new_x = x2/num_k2 <span class="comment">#新的k2的x坐标</span></span><br><span class="line">    k2_new_y = y2/num_k2 <span class="comment">#新的k2的y坐标</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">##新的分类数组</span></span><br><span class="line">    Xk=np.array([k1_new_x,k2_new_x])</span><br><span class="line">    Yk=np.array([k1_new_y,k2_new_y])</span><br><span class="line">    <span class="keyword">return</span> Xk,Yk</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_point</span><span class="params">(Xk,Yk,cls_dict)</span>:</span></span><br><span class="line">    <span class="comment">#画样本点</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>,<span class="number">4</span>)) </span><br><span class="line">    plt.scatter(Xn,Yn,color=<span class="string">"green"</span>,label=<span class="string">"数据"</span>,linewidth=<span class="number">1</span>)</span><br><span class="line">    plt.scatter(Xk,Yk,color=<span class="string">"red"</span>,label=<span class="string">"分类"</span>,linewidth=<span class="number">1</span>)</span><br><span class="line">    plt.xticks(range(<span class="number">1</span>,<span class="number">6</span>))</span><br><span class="line">    plt.xlim([<span class="number">1</span>,<span class="number">5</span>])</span><br><span class="line">    plt.ylim([<span class="number">1</span>,<span class="number">6</span>])</span><br><span class="line">    plt.legend()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xn)):</span><br><span class="line">        plt.text(Xn[i],Yn[i],sign_n[i]+<span class="string">":"</span>+cls_dict[sign_n[i]])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xk)):</span><br><span class="line">            plt.text(Xk[i],Yk[i],sign_k[i])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">##种子</span></span><br><span class="line">    Xk=np.array([<span class="number">3.3</span>,<span class="number">3.0</span>])</span><br><span class="line">    Yk=np.array([<span class="number">5.7</span>,<span class="number">3.2</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        cls_dict =start_class(Xk,Yk)</span><br><span class="line">        Xk_new,Yk_new =recal_class_point(Xk,Yk,cls_dict)</span><br><span class="line">        Xk=Xk_new</span><br><span class="line">        Yk=Yk_new</span><br><span class="line">        draw_point(Xk,Yk,cls_dict)</span><br></pre></td></tr></table></figure>
<p> 最终分类结果：<br> <img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_4.png" class=""></p>
<p> 由上图可以看出，C点最终是属于k1类，而不是开始的k2.</p>
<h3 id="K-Means的不足"><a href="#K-Means的不足" class="headerlink" title="K-Means的不足"></a>K-Means的不足</h3><p> K-Means算法的不足，都是由初始值引起的：</p>
<ol>
<li>初始分类数目k值很难估计，不确定应该分成多少类才最合适(ISODATA算法通过类的自动合并和分裂，得到较为合理的类型数目k。这里不讲这个算法)</li>
<li><p>不同的随机种子会得到完全不同的结果(K-Means++算法可以用来解决这个问题，其可以有效地选择初始点)</p>
<h3 id="K-Means-算法"><a href="#K-Means-算法" class="headerlink" title="K-Means++算法"></a>K-Means++算法</h3><p>算法流程如下：</p>
<ol>
<li>在数据集中随机挑选1个点作为种子点<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##随机挑选一个数据点作为种子点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_seed</span><span class="params">(Xn)</span>:</span></span><br><span class="line">    idx = np.random.choice(range(len(Xn)))</span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
<ol>
<li><p>计算剩余数据点到这个点的距离d(x),并且加入到列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##计算数据点到种子点的距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_dis</span><span class="params">(Xn,Yn,idx)</span>:</span></span><br><span class="line">    dis_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xn)):       </span><br><span class="line">        d = np.sqrt((Xn[i]-Xn[idx])**<span class="number">2</span>+(Yn[i]-Yn[idx])**<span class="number">2</span>)</span><br><span class="line">        dis_list.append(d)</span><br><span class="line">    <span class="keyword">return</span> dis_list</span><br></pre></td></tr></table></figure>
</li>
<li><p>再取一个随机值。这次的选择思路是：先取一个能落在上步计算的距离列表求和后(sum(dis_list))的随机值rom，然后用rom -= d(x)，直到rom&lt;=0，此时的点就是下一个“种子点”</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##随机挑选另外的种子点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_seed_other</span><span class="params">(Xn,Yn,dis_list)</span>:</span></span><br><span class="line">    d_sum = sum(dis_list)</span><br><span class="line">    rom = d_sum * np.random.random()</span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xn)):</span><br><span class="line">        rom -= dis_list[i]</span><br><span class="line">        <span class="keyword">if</span> rom &gt; <span class="number">0</span> :</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            idx = i</span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure></li>
<li>重复第2步和第3步，直到选出k个种子</li>
<li>进行标准的K-Means算法。下面完整代码<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">##样本数据(Xi,Yi)，需要转换成数组(列表)形式</span></span><br><span class="line">Xn=np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">1.9</span>,<span class="number">2.5</span>,<span class="number">4</span>])</span><br><span class="line">Yn=np.array([<span class="number">5</span>,<span class="number">4.8</span>,<span class="number">4</span>,<span class="number">1.8</span>,<span class="number">2.2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#标识符号</span></span><br><span class="line">sign_n = [<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>,<span class="string">'E'</span>]</span><br><span class="line">sign_k = [<span class="string">'k1'</span>,<span class="string">'k2'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">##随机挑选一个数据点作为种子点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_seed</span><span class="params">(Xn)</span>:</span></span><br><span class="line">    idx = np.random.choice(range(len(Xn)))</span><br><span class="line">    <span class="keyword">return</span> idx</span><br><span class="line">    </span><br><span class="line"><span class="comment">##计算数据点到种子点的距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_dis</span><span class="params">(Xn,Yn,idx)</span>:</span></span><br><span class="line">    dis_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xn)):       </span><br><span class="line">        d = np.sqrt((Xn[i]-Xn[idx])**<span class="number">2</span>+(Yn[i]-Yn[idx])**<span class="number">2</span>)</span><br><span class="line">        dis_list.append(d)</span><br><span class="line">    <span class="keyword">return</span> dis_list</span><br><span class="line"></span><br><span class="line"><span class="comment">##随机挑选另外的种子点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_seed_other</span><span class="params">(Xn,Yn,dis_list)</span>:</span></span><br><span class="line">    d_sum = sum(dis_list)</span><br><span class="line">    rom = d_sum * np.random.random()</span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xn)):</span><br><span class="line">        rom -= dis_list[i]</span><br><span class="line">        <span class="keyword">if</span> rom &gt; <span class="number">0</span> :</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            idx = i</span><br><span class="line">    <span class="keyword">return</span> idx</span><br><span class="line"></span><br><span class="line"><span class="comment">##选取所有种子点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_seed_all</span><span class="params">(seed_count)</span>:</span></span><br><span class="line">     <span class="comment">##种子点</span></span><br><span class="line">    Xk = []  <span class="comment">##种子点x轴列表</span></span><br><span class="line">    Yk = []  <span class="comment">##种子点y轴列表</span></span><br><span class="line">    </span><br><span class="line">    idx = <span class="number">0</span>  <span class="comment">##选取的种子点的索引</span></span><br><span class="line">    dis_list = [] <span class="comment">##距离列表</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">##选取种子点</span></span><br><span class="line">    <span class="comment">#因为实验数据少，有一定的几率选到同一个数据，所以加一个判断</span></span><br><span class="line">    idx_list = []</span><br><span class="line">    flag = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(seed_count):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">             idx = select_seed(Xn)</span><br><span class="line">             dis_list = cal_dis(Xn,Yn,idx)</span><br><span class="line">             Xk.append(Xn[idx])</span><br><span class="line">             Yk.append(Yn[idx])</span><br><span class="line">             idx_list.append(idx)</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="keyword">while</span> flag:</span><br><span class="line">                idx = select_seed_other(Xn,Yn,dis_list)</span><br><span class="line">                <span class="keyword">if</span> idx <span class="keyword">not</span> <span class="keyword">in</span> idx_list:</span><br><span class="line">                    flag = <span class="keyword">False</span></span><br><span class="line">                <span class="keyword">else</span> :</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">            dis_list = cal_dis(Xn,Yn,idx)</span><br><span class="line">            Xk.append(Xn[idx])</span><br><span class="line">            Yk.append(Yn[idx])</span><br><span class="line">            idx_list.append(idx)</span><br><span class="line">                </span><br><span class="line">    <span class="comment">##列表转成数组       </span></span><br><span class="line">    Xk=np.array(Xk)</span><br><span class="line">    Yk=np.array(Yk)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Xk,Yk</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_class</span><span class="params">(Xk,Yk)</span>:</span></span><br><span class="line">    <span class="comment">##数据点分类</span></span><br><span class="line">    cls_dict = &#123;&#125;</span><br><span class="line">    <span class="comment">##离哪个分类点最近，属于哪个分类</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xn)):</span><br><span class="line">        temp = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(Xk)):</span><br><span class="line">            d1 = np.sqrt((Xn[i]-Xk[j])*(Xn[i]-Xk[j])+(Yn[i]-Yk[j])*(Yn[i]-Yk[j]))</span><br><span class="line">            temp.append(d1)</span><br><span class="line">        min_dis=np.min(temp)</span><br><span class="line">        min_inx = temp.index(min_dis)</span><br><span class="line">        cls_dict[sign_n[i]]=sign_k[min_inx]</span><br><span class="line">    <span class="comment">#print(cls_dict)</span></span><br><span class="line">    <span class="keyword">return</span> cls_dict</span><br><span class="line">    </span><br><span class="line"><span class="comment">##重新计算分类的坐标点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recal_class_point</span><span class="params">(Xk,Yk,cls_dict)</span>:</span>  </span><br><span class="line">    num_k1 = <span class="number">0</span>  <span class="comment">#属于k1的数据点的个数</span></span><br><span class="line">    num_k2 = <span class="number">0</span>  <span class="comment">#属于k2的数据点的个数</span></span><br><span class="line">    x1 =<span class="number">0</span>       <span class="comment">#属于k1的x坐标和</span></span><br><span class="line">    y1 =<span class="number">0</span>       <span class="comment">#属于k1的y坐标和</span></span><br><span class="line">    x2 =<span class="number">0</span>       <span class="comment">#属于k2的x坐标和</span></span><br><span class="line">    y2 =<span class="number">0</span>       <span class="comment">#属于k2的y坐标和</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">##循环读取已经分类的数据</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> cls_dict:</span><br><span class="line">        <span class="comment">##读取d的类别</span></span><br><span class="line">        kk = cls_dict[d]</span><br><span class="line">        <span class="keyword">if</span> kk == <span class="string">'k1'</span>:</span><br><span class="line">            <span class="comment">#读取d在数据集中的索引</span></span><br><span class="line">            idx = sign_n.index(d)</span><br><span class="line">            <span class="comment">##累加x值</span></span><br><span class="line">            x1 += Xn[idx]</span><br><span class="line">            <span class="comment">##累加y值</span></span><br><span class="line">            y1 += Yn[idx]</span><br><span class="line">            <span class="comment">##累加分类个数</span></span><br><span class="line">            num_k1 += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="comment">#读取d在数据集中的索引</span></span><br><span class="line">            idx = sign_n.index(d)</span><br><span class="line">            <span class="comment">##累加x值</span></span><br><span class="line">            x2 += Xn[idx]</span><br><span class="line">            <span class="comment">##累加y值</span></span><br><span class="line">            y2 += Yn[idx]</span><br><span class="line">            <span class="comment">##累加分类个数</span></span><br><span class="line">            num_k2 += <span class="number">1</span></span><br><span class="line">    <span class="comment">##求平均值获取新的分类坐标点</span></span><br><span class="line">    k1_new_x = x1/num_k1 <span class="comment">#新的k1的x坐标</span></span><br><span class="line">    k1_new_y = y1/num_k1 <span class="comment">#新的k1的y坐标</span></span><br><span class="line"></span><br><span class="line">    k2_new_x = x2/num_k2 <span class="comment">#新的k2的x坐标</span></span><br><span class="line">    k2_new_y = y2/num_k2 <span class="comment">#新的k2的y坐标</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">##新的分类数组</span></span><br><span class="line">    Xk=np.array([k1_new_x,k2_new_x])</span><br><span class="line">    Yk=np.array([k1_new_y,k2_new_y])</span><br><span class="line">    <span class="keyword">return</span> Xk,Yk</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_point</span><span class="params">(Xk,Yk,cls_dict)</span>:</span></span><br><span class="line">    <span class="comment">#画样本点</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>,<span class="number">4</span>)) </span><br><span class="line">    plt.scatter(Xn,Yn,color=<span class="string">"green"</span>,label=<span class="string">"数据"</span>,linewidth=<span class="number">1</span>)</span><br><span class="line">    plt.scatter(Xk,Yk,color=<span class="string">"red"</span>,label=<span class="string">"分类"</span>,linewidth=<span class="number">1</span>)</span><br><span class="line">    plt.xticks(range(<span class="number">1</span>,<span class="number">6</span>))</span><br><span class="line">    plt.xlim([<span class="number">1</span>,<span class="number">5</span>])</span><br><span class="line">    plt.ylim([<span class="number">1</span>,<span class="number">6</span>])</span><br><span class="line">    plt.legend()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xn)):</span><br><span class="line">        plt.text(Xn[i],Yn[i],sign_n[i]+<span class="string">":"</span>+cls_dict[sign_n[i]])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xk)):</span><br><span class="line">            plt.text(Xk[i],Yk[i],sign_k[i])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_point_all_seed</span><span class="params">(Xk,Yk)</span>:</span></span><br><span class="line">    <span class="comment">#画样本点</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>,<span class="number">4</span>)) </span><br><span class="line">    plt.scatter(Xn,Yn,color=<span class="string">"green"</span>,label=<span class="string">"数据"</span>,linewidth=<span class="number">1</span>)</span><br><span class="line">    plt.scatter(Xk,Yk,color=<span class="string">"red"</span>,label=<span class="string">"分类"</span>,linewidth=<span class="number">1</span>)</span><br><span class="line">    plt.xticks(range(<span class="number">1</span>,<span class="number">6</span>))</span><br><span class="line">    plt.xlim([<span class="number">1</span>,<span class="number">5</span>])</span><br><span class="line">    plt.ylim([<span class="number">1</span>,<span class="number">6</span>])</span><br><span class="line">    plt.legend()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xn)):</span><br><span class="line">        plt.text(Xn[i],Yn[i],sign_n[i])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">     <span class="comment">##选取2个种子点</span></span><br><span class="line">     Xk,Yk = select_seed_all(<span class="number">2</span>)</span><br><span class="line">     <span class="comment">##查看种子点</span></span><br><span class="line">     draw_point_all_seed(Xk,Yk)</span><br><span class="line">     <span class="comment">##循环三次进行分类</span></span><br><span class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        cls_dict =start_class(Xk,Yk)</span><br><span class="line">        Xk_new,Yk_new =recal_class_point(Xk,Yk,cls_dict)</span><br><span class="line">        Xk=Xk_new</span><br><span class="line">        Yk=Yk_new</span><br><span class="line">        draw_point(Xk,Yk,cls_dict)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_5.png" class="">
<p>如图所示，选择了A、E两点作为种子点。</p>
<img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_6.png" class="">
<p>最终的结果。</p>
<p>补充说明：因为数据量太少，在选取所有种子函数的while阶段有可能陷入死循环，所以需要关闭代码重新运行才可以出结果。</p>
<h3 id="sklearn包中的K-Means算法"><a href="#sklearn包中的K-Means算法" class="headerlink" title="sklearn包中的K-Means算法"></a>sklearn包中的K-Means算法</h3><ol>
<li>函数：sklearn.cluster.KMeans</li>
<li>主要参数<br> a.  n_clusters：要进行的分类的个数，即上文中k值，默认是8<br> b.  max_iter  ：最大迭代次数。默认300<br> c. min_iter   ：最小迭代次数，默认10<br> d. init：有三个可选项<pre><code> *  &#39;k-means ++&#39;：使用k-means++算法，默认选项
 *  &#39;random&#39;:从初始质心数据中随机选择k个观察值
 *  第三个是数组形式的参数
</code></pre> e. n_jobs: 设置并行量 （-1表示使用所有CPU）</li>
<li>主要属性：<br> a. cluster_centers_ ：集群中心的坐标<br> b. labels_ : 每个点的标签</li>
<li>官网示例：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line"><span class="meta">... </span>              [<span class="number">4</span>, <span class="number">2</span>], [<span class="number">4</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kmeans = KMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">0</span>).fit(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kmeans.labels_</span><br><span class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], dtype=int32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kmeans.predict([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>], dtype=int32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kmeans.cluster_centers_</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">       [ <span class="number">4.</span>,  <span class="number">2.</span>]])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="AP算法"><a href="#AP算法" class="headerlink" title="AP算法"></a>AP算法</h2><h3 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h3><p>AP(Affinity Propagation)通常被翻译为近邻传播算法或者亲和力传播算法，是在2007年的Science杂志上提出的一种新的聚类算法。AP算法的基本思想是将全部数据点都当作潜在的聚类中心(称之为exemplar)，然后数据点两两之间连线构成一个网络(相似度矩阵)，再通过网络中各条边的消息(responsibility和availability)传递计算出各样本的聚类中心。</p>
<h3 id="相关概念-假如有数据点i和数据点j"><a href="#相关概念-假如有数据点i和数据点j" class="headerlink" title="相关概念(假如有数据点i和数据点j)"></a>相关概念(假如有数据点i和数据点j)</h3><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/ap_1.png" class="">
<img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/ap_2.png" class="">
<img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/ap_3.png" class="">
<ol>
<li>相似度： 点j作为点i的聚类中心的能力，记为S(i,j)。一般使用负的欧式距离，所以S(i,j)越大，表示两个点距离越近，相似度也就越高。使用负的欧式距离，相似度是对称的，如果采用其他算法，相似度可能就不是对称的。</li>
<li>相似度矩阵：N个点之间两两计算相似度，这些相似度就组成了相似度矩阵。如图1所示的黄色区域，就是一个5*5的相似度矩阵(N=5)</li>
<li>preference：指点i作为聚类中心的参考度(不能为0)，取值为S对角线的值(图1红色标注部分)，此值越大，最为聚类中心的可能性就越大。但是对角线的值为0，所以需要重新设置对角线的值，既可以根据实际情况设置不同的值，也可以设置成同一值。一般设置为S相似度值的中值。(有的说设置成S的最小值产生的聚类最少，但是在下面的算法中设置成中值产生的聚类是最少的) </li>
<li>Responsibility(吸引度):指点k适合作为数据点i的聚类中心的程度，记为r(i,k)。如图2红色箭头所示，表示点i给点k发送信息，是一个点i选点k的过程。</li>
<li>Availability(归属度):指点i选择点k作为其聚类中心的适合程度，记为a(i,k)。如图3红色箭头所示，表示点k给点i发送信息，是一个点k选点i的过程。</li>
<li>exemplar：指的是聚类中心。</li>
<li>r(i, k)加a(i, k)越大,则k点作为聚类中心的可能性就越大,并且i点隶属于以k点为聚类中心的聚类的可能性也越大</li>
</ol>
<h3 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h3><ol>
<li>吸引度迭代公式:<img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/ap_4.png" class="">
说明1：$R_{t+1}(i,k)$表示新的$R(i,k)$，$Rt(i,k)$表示旧的$R(i,k)$，也许这样说更容易理解。其中λ是阻尼系数，取值[0.5,1)，用于算法的收敛</li>
</ol>
<p>说明2：网上还有另外一种数学公式：<br><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/ap_5.png" class=""></p>
<p>sklearn官网的公式是：<br><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/ap_6.png" class=""><br>我试了这两种公式之后，发现还是公式一的聚类效果最好。同样的数据都采取S的中值作为参考度，我自己写的算法聚类中心是5个，sklearn提供的算法聚类中心是十三个，但是如果把参考度设置为p=-50，则我自己写的算法聚类中心很多，sklearn提供的聚类算法产生标准的3个聚类中心(因为数据是围绕三个中心点产生的)，目前还不清楚这个p=-50是怎么得到的。</p>
<ol>
<li>归属度迭代公式<img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/ap_7.png" class="">
说明：$A_{t+1}(i,k)$表示新的$A(i,k)$，$At(i,k)$表示旧的$A(i,k)$。其中λ是阻尼系数，取值[0.5,1)，用于算法的收敛</li>
</ol>
<h3 id="详细的算法流程"><a href="#详细的算法流程" class="headerlink" title="详细的算法流程"></a>详细的算法流程</h3><ol>
<li><p>设置实验数据。使用sklearn包中提供的函数，随机生成以[1, 1], [-1, -1], [1, -1]三个点为中心的150个数据。   </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_sample</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">## 生成的测试数据的中心点</span></span><br><span class="line">    centers = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">-1</span>]]</span><br><span class="line">    <span class="comment">##生成数据</span></span><br><span class="line">    Xn, labels_true = make_blobs(n_samples=<span class="number">150</span>, centers=centers, cluster_std=<span class="number">0.5</span>,</span><br><span class="line">                            random_state=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#3数据的长度，即：数据点的个数</span></span><br><span class="line">    dataLen = len(Xn)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Xn,dataLen</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算相似度矩阵，并且设置参考度，这里使用相似度矩阵的中值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_simi</span><span class="params">(Xn)</span>:</span></span><br><span class="line">    <span class="comment">##这个数据集的相似度矩阵，最终是二维数组</span></span><br><span class="line">    simi = []</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> Xn:</span><br><span class="line">        <span class="comment">##每个数字与所有数字的相似度列表，即矩阵中的一行</span></span><br><span class="line">        temp = []</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> Xn:</span><br><span class="line">            <span class="comment">##采用负的欧式距离计算相似度</span></span><br><span class="line">            s =-np.sqrt((m[<span class="number">0</span>]-n[<span class="number">0</span>])**<span class="number">2</span> + (m[<span class="number">1</span>]-n[<span class="number">1</span>])**<span class="number">2</span>)</span><br><span class="line">            temp.append(s)</span><br><span class="line">        simi.append(temp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">##设置参考度，即对角线的值，一般为最小值或者中值</span></span><br><span class="line">    <span class="comment">#p = np.min(simi)   ##11个中心</span></span><br><span class="line">    <span class="comment">#p = np.max(simi)  ##14个中心</span></span><br><span class="line">    p = np.median(simi)  <span class="comment">##5个中心</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dataLen):</span><br><span class="line">        simi[i][i] = p</span><br><span class="line">    <span class="keyword">return</span> simi</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算吸引度矩阵，即R值。<br>如果有细心的同学会发现，在上述求R和求A的公式中，求R需要A，求A需要R，所以R或者A不是一开始就可以求解出的，需要先初始化，然后再更新。(我开始就陷入了这个误区，总觉得公式有问题，囧)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##初始化R矩阵、A矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_R</span><span class="params">(dataLen)</span>:</span></span><br><span class="line">    R = [[<span class="number">0</span>]*dataLen <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen)] </span><br><span class="line">    <span class="keyword">return</span> R</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_A</span><span class="params">(dataLen)</span>:</span></span><br><span class="line">    A = [[<span class="number">0</span>]*dataLen <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen)]</span><br><span class="line">    <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line"><span class="comment">##迭代更新R矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iter_update_R</span><span class="params">(dataLen,R,A,simi)</span>:</span></span><br><span class="line">    old_r = <span class="number">0</span> <span class="comment">##更新前的某个r值</span></span><br><span class="line">    lam = <span class="number">0.5</span> <span class="comment">##阻尼系数,用于算法收敛</span></span><br><span class="line">    <span class="comment">##此循环更新R矩阵</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dataLen):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(dataLen):</span><br><span class="line">            old_r = R[i][k]</span><br><span class="line">            <span class="keyword">if</span> i != k:</span><br><span class="line">                max1 = A[i][<span class="number">0</span>] + R[i][<span class="number">0</span>]  <span class="comment">##注意初始值的设置</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen):</span><br><span class="line">                    <span class="keyword">if</span> j != k:</span><br><span class="line">                        <span class="keyword">if</span> A[i][j] + R[i][j] &gt; max1 :</span><br><span class="line">                            max1 = A[i][j] + R[i][j]</span><br><span class="line">                <span class="comment">##更新后的R[i][k]值</span></span><br><span class="line">                R[i][k] = simi[i][k] - max1</span><br><span class="line">                <span class="comment">##带入阻尼系数重新更新</span></span><br><span class="line">                R[i][k] = (<span class="number">1</span>-lam)*R[i][k] +lam*old_r</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                max2 = simi[i][<span class="number">0</span>] <span class="comment">##注意初始值的设置</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen):</span><br><span class="line">                    <span class="keyword">if</span> j != k:</span><br><span class="line">                        <span class="keyword">if</span> simi[i][j] &gt; max2:</span><br><span class="line">                            max2 = simi[i][j]</span><br><span class="line">                <span class="comment">##更新后的R[i][k]值</span></span><br><span class="line">                R[i][k] = simi[i][k] - max2</span><br><span class="line">                <span class="comment">##带入阻尼系数重新更新</span></span><br><span class="line">                R[i][k] = (<span class="number">1</span>-lam)*R[i][k] +lam*old_r</span><br><span class="line">    print(<span class="string">"max_r:"</span>+str(np.max(R)))</span><br><span class="line">    <span class="comment">#print(np.min(R))</span></span><br><span class="line">    <span class="keyword">return</span> R</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算归属度矩阵，即A值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##迭代更新A矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iter_update_A</span><span class="params">(dataLen,R,A)</span>:</span></span><br><span class="line">    old_a = <span class="number">0</span> <span class="comment">##更新前的某个a值</span></span><br><span class="line">    lam = <span class="number">0.5</span> <span class="comment">##阻尼系数,用于算法收敛</span></span><br><span class="line">    <span class="comment">##此循环更新A矩阵</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dataLen):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(dataLen):</span><br><span class="line">            old_a = A[i][k]</span><br><span class="line">            <span class="keyword">if</span> i ==k :</span><br><span class="line">                max3 = R[<span class="number">0</span>][k] <span class="comment">##注意初始值的设置</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen):</span><br><span class="line">                    <span class="keyword">if</span> j != k:</span><br><span class="line">                        <span class="keyword">if</span> R[j][k] &gt; <span class="number">0</span>:</span><br><span class="line">                            max3 += R[j][k]</span><br><span class="line">                        <span class="keyword">else</span> :</span><br><span class="line">                            max3 += <span class="number">0</span></span><br><span class="line">                A[i][k] = max3</span><br><span class="line">                <span class="comment">##带入阻尼系数更新A值</span></span><br><span class="line">                A[i][k] = (<span class="number">1</span>-lam)*A[i][k] +lam*old_a</span><br><span class="line">            <span class="keyword">else</span> :</span><br><span class="line">                max4 = R[<span class="number">0</span>][k] <span class="comment">##注意初始值的设置</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen):</span><br><span class="line">                    <span class="comment">##上图公式中的i!=k 的求和部分</span></span><br><span class="line">                    <span class="keyword">if</span> j != k <span class="keyword">and</span> j != i:</span><br><span class="line">                        <span class="keyword">if</span> R[j][k] &gt; <span class="number">0</span>:</span><br><span class="line">                            max4 += R[j][k]</span><br><span class="line">                        <span class="keyword">else</span> :</span><br><span class="line">                            max4 += <span class="number">0</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">##上图公式中的min部分</span></span><br><span class="line">                <span class="keyword">if</span> R[k][k] + max4 &gt; <span class="number">0</span>:</span><br><span class="line">                    A[i][k] = <span class="number">0</span></span><br><span class="line">                <span class="keyword">else</span> :</span><br><span class="line">                    A[i][k] = R[k][k] + max4</span><br><span class="line">                    </span><br><span class="line">                <span class="comment">##带入阻尼系数更新A值</span></span><br><span class="line">                A[i][k] = (<span class="number">1</span>-lam)*A[i][k] +lam*old_a</span><br><span class="line">    print(<span class="string">"max_a:"</span>+str(np.max(A)))</span><br><span class="line">    <span class="comment">#print(np.min(A))</span></span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure>
</li>
<li><p>迭代更新R值和A值。终止条件是聚类中心在一定程度上不再更新或者达到最大迭代次数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##计算聚类中心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_cls_center</span><span class="params">(dataLen,simi,R,A)</span>:</span></span><br><span class="line">    <span class="comment">##进行聚类，不断迭代直到预设的迭代次数或者判断comp_cnt次后聚类中心不再变化</span></span><br><span class="line">    max_iter = <span class="number">100</span>    <span class="comment">##最大迭代次数</span></span><br><span class="line">    curr_iter = <span class="number">0</span>     <span class="comment">##当前迭代次数</span></span><br><span class="line">    max_comp = <span class="number">30</span>     <span class="comment">##最大比较次数</span></span><br><span class="line">    curr_comp = <span class="number">0</span>     <span class="comment">##当前比较次数</span></span><br><span class="line">    class_cen = []    <span class="comment">##聚类中心列表，存储的是数据点在Xn中的索引</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="comment">##计算R矩阵</span></span><br><span class="line">        R = iter_update_R(dataLen,R,A,simi)</span><br><span class="line">        <span class="comment">##计算A矩阵</span></span><br><span class="line">        A = iter_update_A(dataLen,R,A)</span><br><span class="line">        <span class="comment">##开始计算聚类中心</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(dataLen):</span><br><span class="line">            <span class="keyword">if</span> R[k][k] +A[k][k] &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> class_cen:</span><br><span class="line">                    class_cen.append(k)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    curr_comp += <span class="number">1</span></span><br><span class="line">        curr_iter += <span class="number">1</span></span><br><span class="line">        print(curr_iter)</span><br><span class="line">        <span class="keyword">if</span> curr_iter &gt;= max_iter <span class="keyword">or</span> curr_comp &gt; max_comp :</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> class_cen</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据求出的聚类中心，对数据进行分类<br>这个步骤产生的是一个归类列表，列表中的每个数字对应着样本数据中对应位置的数据的分类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##根据聚类中心划分数据</span></span><br><span class="line">   c_list = []</span><br><span class="line">   <span class="keyword">for</span> m <span class="keyword">in</span> Xn:</span><br><span class="line">       temp = []</span><br><span class="line">       <span class="keyword">for</span> j <span class="keyword">in</span> class_cen:</span><br><span class="line">           n = Xn[j]</span><br><span class="line">           d = -np.sqrt((m[<span class="number">0</span>]-n[<span class="number">0</span>])**<span class="number">2</span> + (m[<span class="number">1</span>]-n[<span class="number">1</span>])**<span class="number">2</span>)</span><br><span class="line">           temp.append(d)</span><br><span class="line">       <span class="comment">##按照是第几个数字作为聚类中心进行分类标识</span></span><br><span class="line">       c = class_cen[temp.index(np.max(temp))]</span><br><span class="line">       c_list.append(c)</span><br></pre></td></tr></table></figure>
</li>
<li><p>完整代码及效果图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">第一步：生成测试数据</span></span><br><span class="line"><span class="string">    1.生成实际中心为centers的测试样本300个，</span></span><br><span class="line"><span class="string">    2.Xn是包含150个(x,y)点的二维数组</span></span><br><span class="line"><span class="string">    3.labels_true为其对应的真是类别标签</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_sample</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">## 生成的测试数据的中心点</span></span><br><span class="line">    centers = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">-1</span>]]</span><br><span class="line">    <span class="comment">##生成数据</span></span><br><span class="line">    Xn, labels_true = make_blobs(n_samples=<span class="number">150</span>, centers=centers, cluster_std=<span class="number">0.5</span>,</span><br><span class="line">                            random_state=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#3数据的长度，即：数据点的个数</span></span><br><span class="line">    dataLen = len(Xn)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Xn,dataLen</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">第二步：计算相似度矩阵</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_simi</span><span class="params">(Xn)</span>:</span></span><br><span class="line">    <span class="comment">##这个数据集的相似度矩阵，最终是二维数组</span></span><br><span class="line">    simi = []</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> Xn:</span><br><span class="line">        <span class="comment">##每个数字与所有数字的相似度列表，即矩阵中的一行</span></span><br><span class="line">        temp = []</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> Xn:</span><br><span class="line">            <span class="comment">##采用负的欧式距离计算相似度</span></span><br><span class="line">            s =-np.sqrt((m[<span class="number">0</span>]-n[<span class="number">0</span>])**<span class="number">2</span> + (m[<span class="number">1</span>]-n[<span class="number">1</span>])**<span class="number">2</span>)</span><br><span class="line">            temp.append(s)</span><br><span class="line">        simi.append(temp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">##设置参考度，即对角线的值，一般为最小值或者中值</span></span><br><span class="line">    <span class="comment">#p = np.min(simi)   ##11个中心</span></span><br><span class="line">    <span class="comment">#p = np.max(simi)  ##14个中心</span></span><br><span class="line">    p = np.median(simi)  <span class="comment">##5个中心</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dataLen):</span><br><span class="line">        simi[i][i] = p</span><br><span class="line">    <span class="keyword">return</span> simi</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">第三步：计算吸引度矩阵，即R</span></span><br><span class="line"><span class="string">       公式1：r(n+1) =s(n)-(s(n)+a(n))--&gt;简化写法，具体参见上图公式</span></span><br><span class="line"><span class="string">       公式2：r(n+1)=(1-λ)*r(n+1)+λ*r(n)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##初始化R矩阵、A矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_R</span><span class="params">(dataLen)</span>:</span></span><br><span class="line">    R = [[<span class="number">0</span>]*dataLen <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen)] </span><br><span class="line">    <span class="keyword">return</span> R</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_A</span><span class="params">(dataLen)</span>:</span></span><br><span class="line">    A = [[<span class="number">0</span>]*dataLen <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen)]</span><br><span class="line">    <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line"><span class="comment">##迭代更新R矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iter_update_R</span><span class="params">(dataLen,R,A,simi)</span>:</span></span><br><span class="line">    old_r = <span class="number">0</span> <span class="comment">##更新前的某个r值</span></span><br><span class="line">    lam = <span class="number">0.5</span> <span class="comment">##阻尼系数,用于算法收敛</span></span><br><span class="line">    <span class="comment">##此循环更新R矩阵</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dataLen):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(dataLen):</span><br><span class="line">            old_r = R[i][k]</span><br><span class="line">            <span class="keyword">if</span> i != k:</span><br><span class="line">                max1 = A[i][<span class="number">0</span>] + R[i][<span class="number">0</span>]  <span class="comment">##注意初始值的设置</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen):</span><br><span class="line">                    <span class="keyword">if</span> j != k:</span><br><span class="line">                        <span class="keyword">if</span> A[i][j] + R[i][j] &gt; max1 :</span><br><span class="line">                            max1 = A[i][j] + R[i][j]</span><br><span class="line">                <span class="comment">##更新后的R[i][k]值</span></span><br><span class="line">                R[i][k] = simi[i][k] - max1</span><br><span class="line">                <span class="comment">##带入阻尼系数重新更新</span></span><br><span class="line">                R[i][k] = (<span class="number">1</span>-lam)*R[i][k] +lam*old_r</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                max2 = simi[i][<span class="number">0</span>] <span class="comment">##注意初始值的设置</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen):</span><br><span class="line">                    <span class="keyword">if</span> j != k:</span><br><span class="line">                        <span class="keyword">if</span> simi[i][j] &gt; max2:</span><br><span class="line">                            max2 = simi[i][j]</span><br><span class="line">                <span class="comment">##更新后的R[i][k]值</span></span><br><span class="line">                R[i][k] = simi[i][k] - max2</span><br><span class="line">                <span class="comment">##带入阻尼系数重新更新</span></span><br><span class="line">                R[i][k] = (<span class="number">1</span>-lam)*R[i][k] +lam*old_r</span><br><span class="line">    print(<span class="string">"max_r:"</span>+str(np.max(R)))</span><br><span class="line">    <span class="comment">#print(np.min(R))</span></span><br><span class="line">    <span class="keyword">return</span> R</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    第四步：计算归属度矩阵，即A</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">##迭代更新A矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iter_update_A</span><span class="params">(dataLen,R,A)</span>:</span></span><br><span class="line">    old_a = <span class="number">0</span> <span class="comment">##更新前的某个a值</span></span><br><span class="line">    lam = <span class="number">0.5</span> <span class="comment">##阻尼系数,用于算法收敛</span></span><br><span class="line">    <span class="comment">##此循环更新A矩阵</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dataLen):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(dataLen):</span><br><span class="line">            old_a = A[i][k]</span><br><span class="line">            <span class="keyword">if</span> i ==k :</span><br><span class="line">                max3 = R[<span class="number">0</span>][k] <span class="comment">##注意初始值的设置</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen):</span><br><span class="line">                    <span class="keyword">if</span> j != k:</span><br><span class="line">                        <span class="keyword">if</span> R[j][k] &gt; <span class="number">0</span>:</span><br><span class="line">                            max3 += R[j][k]</span><br><span class="line">                        <span class="keyword">else</span> :</span><br><span class="line">                            max3 += <span class="number">0</span></span><br><span class="line">                A[i][k] = max3</span><br><span class="line">                <span class="comment">##带入阻尼系数更新A值</span></span><br><span class="line">                A[i][k] = (<span class="number">1</span>-lam)*A[i][k] +lam*old_a</span><br><span class="line">            <span class="keyword">else</span> :</span><br><span class="line">                max4 = R[<span class="number">0</span>][k] <span class="comment">##注意初始值的设置</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(dataLen):</span><br><span class="line">                    <span class="comment">##上图公式中的i!=k 的求和部分</span></span><br><span class="line">                    <span class="keyword">if</span> j != k <span class="keyword">and</span> j != i:</span><br><span class="line">                        <span class="keyword">if</span> R[j][k] &gt; <span class="number">0</span>:</span><br><span class="line">                            max4 += R[j][k]</span><br><span class="line">                        <span class="keyword">else</span> :</span><br><span class="line">                            max4 += <span class="number">0</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">##上图公式中的min部分</span></span><br><span class="line">                <span class="keyword">if</span> R[k][k] + max4 &gt; <span class="number">0</span>:</span><br><span class="line">                    A[i][k] = <span class="number">0</span></span><br><span class="line">                <span class="keyword">else</span> :</span><br><span class="line">                    A[i][k] = R[k][k] + max4</span><br><span class="line">                    </span><br><span class="line">                <span class="comment">##带入阻尼系数更新A值</span></span><br><span class="line">                A[i][k] = (<span class="number">1</span>-lam)*A[i][k] +lam*old_a</span><br><span class="line">    print(<span class="string">"max_a:"</span>+str(np.max(A)))</span><br><span class="line">    <span class="comment">#print(np.min(A))</span></span><br><span class="line">    <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">   第5步：计算聚类中心</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##计算聚类中心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_cls_center</span><span class="params">(dataLen,simi,R,A)</span>:</span></span><br><span class="line">    <span class="comment">##进行聚类，不断迭代直到预设的迭代次数或者判断comp_cnt次后聚类中心不再变化</span></span><br><span class="line">    max_iter = <span class="number">100</span>    <span class="comment">##最大迭代次数</span></span><br><span class="line">    curr_iter = <span class="number">0</span>     <span class="comment">##当前迭代次数</span></span><br><span class="line">    max_comp = <span class="number">30</span>     <span class="comment">##最大比较次数</span></span><br><span class="line">    curr_comp = <span class="number">0</span>     <span class="comment">##当前比较次数</span></span><br><span class="line">    class_cen = []    <span class="comment">##聚类中心列表，存储的是数据点在Xn中的索引</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="comment">##计算R矩阵</span></span><br><span class="line">        R = iter_update_R(dataLen,R,A,simi)</span><br><span class="line">        <span class="comment">##计算A矩阵</span></span><br><span class="line">        A = iter_update_A(dataLen,R,A)</span><br><span class="line">        <span class="comment">##开始计算聚类中心</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(dataLen):</span><br><span class="line">            <span class="keyword">if</span> R[k][k] +A[k][k] &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> class_cen:</span><br><span class="line">                    class_cen.append(k)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    curr_comp += <span class="number">1</span></span><br><span class="line">        curr_iter += <span class="number">1</span></span><br><span class="line">        print(curr_iter)</span><br><span class="line">        <span class="keyword">if</span> curr_iter &gt;= max_iter <span class="keyword">or</span> curr_comp &gt; max_comp :</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> class_cen</span><br><span class="line">  </span><br><span class="line">   </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">##初始化数据</span></span><br><span class="line">    Xn,dataLen = init_sample()</span><br><span class="line">    <span class="comment">##初始化R、A矩阵</span></span><br><span class="line">    R = init_R(dataLen)</span><br><span class="line">    A = init_A(dataLen)</span><br><span class="line">    <span class="comment">##计算相似度</span></span><br><span class="line">    simi = cal_simi(Xn)   </span><br><span class="line">    <span class="comment">##输出聚类中心</span></span><br><span class="line">    class_cen = cal_cls_center(dataLen,simi,R,A)</span><br><span class="line">    <span class="comment">#for i in class_cen:</span></span><br><span class="line">    <span class="comment">#    print(str(i)+":"+str(Xn[i]))</span></span><br><span class="line">    <span class="comment">#print(class_cen)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">##根据聚类中心划分数据</span></span><br><span class="line">    c_list = []</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> Xn:</span><br><span class="line">        temp = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> class_cen:</span><br><span class="line">            n = Xn[j]</span><br><span class="line">            d = -np.sqrt((m[<span class="number">0</span>]-n[<span class="number">0</span>])**<span class="number">2</span> + (m[<span class="number">1</span>]-n[<span class="number">1</span>])**<span class="number">2</span>)</span><br><span class="line">            temp.append(d)</span><br><span class="line">        <span class="comment">##按照是第几个数字作为聚类中心进行分类标识</span></span><br><span class="line">        c = class_cen[temp.index(np.max(temp))]</span><br><span class="line">        c_list.append(c)</span><br><span class="line">    <span class="comment">##画图</span></span><br><span class="line">    colors = [<span class="string">'red'</span>,<span class="string">'blue'</span>,<span class="string">'black'</span>,<span class="string">'green'</span>,<span class="string">'yellow'</span>]</span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">    plt.xlim([<span class="number">-3</span>,<span class="number">3</span>])</span><br><span class="line">    plt.ylim([<span class="number">-3</span>,<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dataLen):</span><br><span class="line">        d1 = Xn[i]</span><br><span class="line">        d2 = Xn[c_list[i]]</span><br><span class="line">        c = class_cen.index(c_list[i])</span><br><span class="line">        plt.plot([d2[<span class="number">0</span>],d1[<span class="number">0</span>]],[d2[<span class="number">1</span>],d1[<span class="number">1</span>]],color=colors[c],linewidth=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#if i == c_list[i] :</span></span><br><span class="line">        <span class="comment">#    plt.scatter(d1[0],d1[1],color=colors[c],linewidth=3)</span></span><br><span class="line">        <span class="comment">#else :</span></span><br><span class="line">        <span class="comment">#    plt.scatter(d1[0],d1[1],color=colors[c],linewidth=1)</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>迭代11次出结果：<br><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/ap_8.png" class=""></p>
<p>补充说明：这个算法重点在讲解实现过程，执行效率不是特别高，有优化的空间。以后我会补充进来</p>
<h3 id="sklearn包中的AP算法"><a href="#sklearn包中的AP算法" class="headerlink" title="sklearn包中的AP算法"></a>sklearn包中的AP算法</h3><ol>
<li>函数：sklearn.cluster.AffinityPropagation</li>
<li>主要参数：<br> a.  damping : 阻尼系数，取值[0.5,1)<br> b. convergence_iter ：比较多少次聚类中心不变之后停止迭代，默认15<br> c. max_iter ：最大迭代次数<br> d.  preference :参考度</li>
<li>主要属性<br> a.  cluster_centers_indices_ : 存放聚类中心的数组<br> b.  labels_ :存放每个点的分类的数组<br> c.  n_iter_ : 迭代次数</li>
<li>示例<br>preference(即p值)取不同值时的聚类中心的数目在代码中注明了。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AffinityPropagation</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成的测试数据的中心点</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">-1</span>]]</span><br><span class="line"><span class="comment">##生成数据</span></span><br><span class="line">Xn, labels_true = make_blobs(n_samples=<span class="number">150</span>, centers=centers, cluster_std=<span class="number">0.5</span>,</span><br><span class="line">                            random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">simi = []</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> Xn:</span><br><span class="line">    <span class="comment">##每个数字与所有数字的相似度列表，即矩阵中的一行</span></span><br><span class="line">    temp = []</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> Xn:</span><br><span class="line">         <span class="comment">##采用负的欧式距离计算相似度</span></span><br><span class="line">        s =-np.sqrt((m[<span class="number">0</span>]-n[<span class="number">0</span>])**<span class="number">2</span> + (m[<span class="number">1</span>]-n[<span class="number">1</span>])**<span class="number">2</span>)</span><br><span class="line">        temp.append(s)</span><br><span class="line">    simi.append(temp)</span><br><span class="line"></span><br><span class="line">p=<span class="number">-50</span>   <span class="comment">##3个中心</span></span><br><span class="line"><span class="comment">#p = np.min(simi)  ##9个中心，</span></span><br><span class="line"><span class="comment">#p = np.median(simi)  ##13个中心    </span></span><br><span class="line"></span><br><span class="line">ap = AffinityPropagation(damping=<span class="number">0.5</span>,max_iter=<span class="number">500</span>,convergence_iter=<span class="number">30</span>,</span><br><span class="line">                         preference=p).fit(Xn)</span><br><span class="line">cluster_centers_indices = ap.cluster_centers_indices_</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> cluster_centers_indices:</span><br><span class="line">    print(Xn[idx])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="AP算法的优点"><a href="#AP算法的优点" class="headerlink" title="AP算法的优点"></a>AP算法的优点</h3><ol>
<li>不需要制定最终聚类族的个数 </li>
<li>已有的数据点作为最终的聚类中心，而不是新生成一个族中心。 </li>
<li>模型对数据的初始值不敏感。 </li>
<li>对初始相似度矩阵数据的对称性没有要求。 </li>
<li>相比与k-centers聚类方法，其结果的平方差误差较小。</li>
</ol>
<h3 id="AP算法的不足"><a href="#AP算法的不足" class="headerlink" title="AP算法的不足"></a>AP算法的不足</h3><ol>
<li>AP算法需要事先计算每对数据对象之间的相似度，如果数据对象太多的话，内存放不下，若存在数据库，频繁访问数据库也需要时间。</li>
<li>AP算法的时间复杂度较高，一次迭代大概O(N3)</li>
<li>聚类的好坏受到参考度和阻尼系数的影响。</li>
</ol>
<h2 id="Mean-shift"><a href="#Mean-shift" class="headerlink" title="Mean-shift"></a>Mean-shift</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p> Mean-shift（即：均值迁移）的基本思想：在数据集中选定一个点，然后以这个点为圆心，r为半径，画一个圆(二维下是圆)，求出这个点到所有点的向量的平均值，而圆心与向量均值的和为新的圆心，然后迭代此过程，直到满足一点的条件结束。(Fukunage在1975年提出)</p>
<p>后来Yizong Cheng 在此基础上加入了 核函数 和 权重系数 ，使得Mean-shift 算法开始流行起来。目前它在聚类、图像平滑、分割、跟踪等方面有着广泛的应用。</p>
<h3 id="图解过程"><a href="#图解过程" class="headerlink" title="图解过程"></a>图解过程</h3><p>为了方便大家理解，借用下几张图来说明Mean-shift的基本过程。<br></p>
<p>由上图可以很容易看到，Mean-shift 算法的核心思想就是不断的寻找新的圆心坐标，直到密度最大的区域。</p>
<h3 id="Mean-shift-算法函数"><a href="#Mean-shift-算法函数" class="headerlink" title="Mean-shift 算法函数"></a>Mean-shift 算法函数</h3><ol>
<li><p>核心函数：sklearn.cluster.MeanShift(核函数：RBF核函数)<br>由上图可知，圆心(或种子)的确定和半径(或带宽)的选择，是影响算法效率的两个主要因素。所以在sklearn.cluster.MeanShift中重点说明了这两个参数的设定问题。</p>
</li>
<li><p>主要参数<br> a.  bandwidth ：半径(或带宽)，float型。如果没有给出，则使用sklearn.cluster.estimate_bandwidth计算出半径(带宽).（可选）<br> b. seeds :圆心（或种子），数组类型，即初始化的圆心。（可选）<br> c. bin_seeding ：布尔值。如果为真，初始内核位置不是所有点的位置，而是点的离散版本的位置，其中点被分类到其粗糙度对应于带宽的网格上。将此选项设置为True将加速算法，因为较少的种子将被初始化。默认值：False.如果种子参数(seeds)不为None则忽略。</p>
</li>
<li>主要属性<br> a. cluster_centers_ : 数组类型。计算出的聚类中心的坐标。<br> b. labels_ :数组类型。每个数据点的分类标签。</li>
<li>算法示例：代码中有详细讲解内容</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MeanShift, estimate_bandwidth</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle  <span class="comment">##python自带的迭代器模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##产生随机数据的中心</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">-1</span>]]</span><br><span class="line"><span class="comment">##产生的数据个数</span></span><br><span class="line">n_samples=<span class="number">10000</span></span><br><span class="line"><span class="comment">##生产数据</span></span><br><span class="line">X, _ = make_blobs(n_samples=n_samples, centers= centers, cluster_std=<span class="number">0.6</span>, </span><br><span class="line">                  random_state =<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##带宽，也就是以某个点为核心时的搜索半径</span></span><br><span class="line">bandwidth = estimate_bandwidth(X, quantile=<span class="number">0.2</span>, n_samples=<span class="number">500</span>)</span><br><span class="line"><span class="comment">##设置均值偏移函数</span></span><br><span class="line">ms = MeanShift(bandwidth=bandwidth, bin_seeding=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">##训练数据</span></span><br><span class="line">ms.fit(X)</span><br><span class="line"><span class="comment">##每个点的标签</span></span><br><span class="line">labels = ms.labels_</span><br><span class="line">print(labels)</span><br><span class="line"><span class="comment">##簇中心的点的集合</span></span><br><span class="line">cluster_centers = ms.cluster_centers_</span><br><span class="line"><span class="comment">##总共的标签分类</span></span><br><span class="line">labels_unique = np.unique(labels)</span><br><span class="line"><span class="comment">##聚簇的个数，即分类的个数</span></span><br><span class="line">n_clusters_ = len(labels_unique)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"number of estimated clusters : %d"</span> % n_clusters_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">colors = cycle(<span class="string">'bgrcmykbgrcmykbgrcmykbgrcmyk'</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> zip(range(n_clusters_), colors):</span><br><span class="line">    <span class="comment">##根据lables中的值是否等于k，重新组成一个True、False的数组</span></span><br><span class="line">    my_members = labels == k</span><br><span class="line">    cluster_center = cluster_centers[k]</span><br><span class="line">    <span class="comment">##X[my_members, 0] 取出my_members对应位置为True的值的横坐标</span></span><br><span class="line">    plt.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], col + <span class="string">'.'</span>)</span><br><span class="line">    plt.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">'o'</span>, markerfacecolor=col,</span><br><span class="line">             markeredgecolor=<span class="string">'k'</span>, markersize=<span class="number">14</span>)</span><br><span class="line">plt.title(<span class="string">'Estimated number of clusters: %d'</span> % n_clusters_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ol>
<li>效果图<img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/mean-shift_2.png" class="">
</li>
</ol>
<h2 id="Spectral-Clustering"><a href="#Spectral-Clustering" class="headerlink" title="Spectral Clustering"></a>Spectral Clustering</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>Spectral Clustering(SC,即谱聚类)，是一种基于图论的聚类方法,它能够识别任意形状的样本空间且收敛于全局最优解，其基本思想是利用样本数据的相似矩阵进行特征分解后得到的特征向量进行聚类.它与样本特征无关而只与样本个数有关。</p>
<p>基本思路：将样本看作顶点,样本间的相似度看作带权的边,从而将聚类问题转为图分割问题:找到一种图分割的方法使得连接不同组的边的权重尽可能低(这意味着组间相似度要尽可能低),组内的边的权重尽可能高(这意味着组内相似度要尽可能高).</p>
<h3 id="图解过程-1"><a href="#图解过程-1" class="headerlink" title="图解过程"></a>图解过程</h3><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sc_1.png" class="">
<p>如上图所示，断开虚线，六个数据被聚成两类。</p>
<h2 id="Spectral-Clustering算法函数"><a href="#Spectral-Clustering算法函数" class="headerlink" title="Spectral Clustering算法函数"></a>Spectral Clustering算法函数</h2><h3 id="核心函数：sklearn-cluster-SpectralClustering"><a href="#核心函数：sklearn-cluster-SpectralClustering" class="headerlink" title="核心函数：sklearn.cluster.SpectralClustering"></a>核心函数：sklearn.cluster.SpectralClustering</h3><p>因为是基于图论的算法，所以输入必须是对称矩阵。</p>
<h3 id="主要参数-参数较多，详细参数"><a href="#主要参数-参数较多，详细参数" class="headerlink" title="主要参数(参数较多，详细参数)"></a>主要参数(参数较多，<a href="http://scikitlearn.org/dev/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" target="_blank" rel="noopener">详细参数</a>)</h3><pre><code>1. n_clusters：聚类的个数。（官方的解释：投影子空间的维度）
2. affinity：核函数，默认是&#39;rbf&#39;，可选：&quot;nearest_neighbors&quot;，&quot;precomputed&quot;,&quot;rbf&quot;或sklearn.metrics.pairwise_kernels支持的其中一个内核之一。
3. gamma :affinity指定的核函数的内核系数，默认1.0
</code></pre><h3 id="主要属性"><a href="#主要属性" class="headerlink" title="主要属性"></a>主要属性</h3><p>labels_ ：每个数据的分类标签</p>
<h3 id="算法示例：代码中有详细讲解内容"><a href="#算法示例：代码中有详细讲解内容" class="headerlink" title="算法示例：代码中有详细讲解内容"></a>算法示例：代码中有详细讲解内容</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> spectral_clustering</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle  <span class="comment">##python自带的迭代器模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##产生随机数据的中心</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">-1</span>]]</span><br><span class="line"><span class="comment">##产生的数据个数</span></span><br><span class="line">n_samples=<span class="number">3000</span></span><br><span class="line"><span class="comment">##生产数据</span></span><br><span class="line">X, lables_true = make_blobs(n_samples=n_samples, centers= centers, cluster_std=<span class="number">0.6</span>, </span><br><span class="line">                  random_state =<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##变换成矩阵，输入必须是对称矩阵</span></span><br><span class="line">metrics_metrix = (<span class="number">-1</span> * metrics.pairwise.pairwise_distances(X)).astype(np.int32)</span><br><span class="line">metrics_metrix += <span class="number">-1</span> * metrics_metrix.min()</span><br><span class="line"><span class="comment">##设置谱聚类函数</span></span><br><span class="line">n_clusters_= <span class="number">4</span></span><br><span class="line">lables = spectral_clustering(metrics_metrix,n_clusters=n_clusters_)</span><br><span class="line"></span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">colors = cycle(<span class="string">'bgrcmykbgrcmykbgrcmykbgrcmyk'</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> zip(range(n_clusters_), colors):</span><br><span class="line">    <span class="comment">##根据lables中的值是否等于k，重新组成一个True、False的数组</span></span><br><span class="line">    my_members = lables == k</span><br><span class="line">    <span class="comment">##X[my_members, 0] 取出my_members对应位置为True的值的横坐标</span></span><br><span class="line">    plt.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], col + <span class="string">'.'</span>)</span><br><span class="line">    </span><br><span class="line">plt.title(<span class="string">'Estimated number of clusters: %d'</span> % n_clusters_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h3><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sc_2.png" class="">
<h2 id="Hierarchical-Clustering"><a href="#Hierarchical-Clustering" class="headerlink" title="Hierarchical Clustering"></a>Hierarchical Clustering</h2><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><p>Hierarchical Clustering(层次聚类)：就是按照某种方法进行层次分类，直到满足某种条件为止。<br>主要分成两类：<br>a. 凝聚：从下到上。首先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到所有的对象都在一个簇中，或者某个终结条件被满足。<br>b. 分裂：从上到下。首先将所有对象置于同一个簇中，然后逐渐细分为越来越小的簇，直到每个对象自成一簇，或者达到了某个终止条件。（较少用）</p>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol>
<li>将每个对象归为一类, 共得到N类, 每类仅包含一个对象. 类与类之间的距离就是它们所包含的对象之间的距离.</li>
<li>找到最接近的两个类并合并成一类, 于是总的类数少了一个.</li>
<li>重新计算新的类与所有旧类之间的距离.</li>
<li>重复第2步和第3步, 直到最后合并成一个类为止(此类包含了N个对象).</li>
</ol>
<h3 id="图解过程-2"><a href="#图解过程-2" class="headerlink" title="图解过程"></a>图解过程</h3><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/hc_1.png" class="">
<h3 id="Hierarchical-Clustering算法函数"><a href="#Hierarchical-Clustering算法函数" class="headerlink" title="Hierarchical Clustering算法函数"></a>Hierarchical Clustering算法函数</h3><ol>
<li>sklearn.cluster.AgglomerativeClustering</li>
<li><p>主要参数(<a href="http://scikit-learn.org/dev/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" target="_blank" rel="noopener">详细参数</a>)</p>
<p>a. n_clusters：聚类的个数<br>b. linkage：指定层次聚类判断相似度的方法，有以下三种：</p>
<ul>
<li>ward：组间距离等于两类对象之间的最小距离。（即single-linkage聚类）</li>
<li>average：组间距离等于两组对象之间的平均距离。（average-linkage聚类）</li>
<li>complete：组间距离等于两组对象之间的最大距离。（complete-linkage聚类）</li>
</ul>
</li>
<li><p>主要属性<br>labels_： 每个数据的分类标签</p>
</li>
<li>算法示例：代码中有详细讲解内容<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle  <span class="comment">##python自带的迭代器模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##产生随机数据的中心</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">-1</span>]]</span><br><span class="line"><span class="comment">##产生的数据个数</span></span><br><span class="line">n_samples=<span class="number">3000</span></span><br><span class="line"><span class="comment">##生产数据</span></span><br><span class="line">X, lables_true = make_blobs(n_samples=n_samples, centers= centers, cluster_std=<span class="number">0.6</span>, </span><br><span class="line">                  random_state =<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##设置分层聚类函数</span></span><br><span class="line">linkages = [<span class="string">'ward'</span>, <span class="string">'average'</span>, <span class="string">'complete'</span>]</span><br><span class="line">n_clusters_ = <span class="number">3</span></span><br><span class="line">ac = AgglomerativeClustering(linkage=linkages[<span class="number">2</span>],n_clusters = n_clusters_)</span><br><span class="line"><span class="comment">##训练数据</span></span><br><span class="line">ac.fit(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">##每个数据的分类</span></span><br><span class="line">lables = ac.labels_</span><br><span class="line"></span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">colors = cycle(<span class="string">'bgrcmykbgrcmykbgrcmykbgrcmyk'</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> zip(range(n_clusters_), colors):</span><br><span class="line">    <span class="comment">##根据lables中的值是否等于k，重新组成一个True、False的数组</span></span><br><span class="line">    my_members = lables == k</span><br><span class="line">    <span class="comment">##X[my_members, 0] 取出my_members对应位置为True的值的横坐标</span></span><br><span class="line">    plt.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], col + <span class="string">'.'</span>)</span><br><span class="line">    </span><br><span class="line">plt.title(<span class="string">'Estimated number of clusters: %d'</span> % n_clusters_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li>
<li>效果图：参数linkage的取值依次为：[‘ward’, ‘average’, ‘complete’]<img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/hc_2.png" class="">
</li>
</ol>
<h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><h3 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h3><p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise,具有噪声的基于密度的聚类方法）是一种基于密度的空间聚类算法。该算法将具有足够密度的区域划分为簇(即要求聚类空间中的一定区域内所包含对象的数目不小于某一给定阈值)，并在具有噪声的空间数据库中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。</p>
<h3 id="算法步骤（大致非详细）"><a href="#算法步骤（大致非详细）" class="headerlink" title="算法步骤（大致非详细）"></a>算法步骤（大致非详细）</h3><p>DBSCAN需要二个参数:扫描半径 (eps)和最小包含点数(min_samples)</p>
<ol>
<li>遍历所有点，寻找核心点</li>
<li>连通核心点，并且在此过程中扩展某个分类集合中点的个数</li>
</ol>
<h3 id="图解过程-3"><a href="#图解过程-3" class="headerlink" title="图解过程"></a>图解过程</h3><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/dbscan_1.png" class="">
<p>在上图中，第一步就是寻找红色的核心点，第二步就是用绿色箭头联通红色点。图中点以绿色线条为中心被分成了两类。没在黑色圆中的点是噪声点。</p>
<h3 id="DBSCAN算法函数"><a href="#DBSCAN算法函数" class="headerlink" title="DBSCAN算法函数"></a>DBSCAN算法函数</h3><ol>
<li>sklearn.cluster.DBSCAN</li>
<li>主要参数（<a href="http://scikit-learn.org/dev/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN" target="_blank" rel="noopener">详细参数</a>）<br> a. eps:两个样本之间的最大距离，即扫描半径<br> b. min_samples ：作为核心点的话邻域(即以其为圆心，eps为半径的圆，含圆上的点)中的最小样本数(包括点本身)。</li>
<li>主要属性<br> a. core_sample_indices_:核心样本指数。（此参数在代码中有详细的解释）<br> b. labels_:数据集中每个点的集合标签给,噪声点标签为-1。</li>
<li>算法示例：代码中有详细讲解内容<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle  <span class="comment">##python自带的迭代器模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment">##产生随机数据的中心</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">-1</span>]]</span><br><span class="line"><span class="comment">##产生的数据个数</span></span><br><span class="line">n_samples=<span class="number">750</span></span><br><span class="line"><span class="comment">##生产数据:此实验结果受cluster_std的影响，或者说受eps 和cluster_std差值影响</span></span><br><span class="line">X, lables_true = make_blobs(n_samples=n_samples, centers= centers, cluster_std=<span class="number">0.4</span>, </span><br><span class="line">                  random_state =<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##设置分层聚类函数</span></span><br><span class="line">db = DBSCAN(eps=<span class="number">0.3</span>, min_samples=<span class="number">10</span>)</span><br><span class="line"><span class="comment">##训练数据</span></span><br><span class="line">db.fit(X)</span><br><span class="line"><span class="comment">##初始化一个全是False的bool类型的数组</span></span><br><span class="line">core_samples_mask = np.zeros_like(db.labels_, dtype=bool)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">   这里是关键点(针对这行代码：xy = X[class_member_mask &amp; ~core_samples_mask])：</span></span><br><span class="line"><span class="string">   db.core_sample_indices_  表示的是某个点在寻找核心点集合的过程中暂时被标为噪声点的点(即周围点</span></span><br><span class="line"><span class="string">   小于min_samples)，并不是最终的噪声点。在对核心点进行联通的过程中，这部分点会被进行重新归类(即标签</span></span><br><span class="line"><span class="string">   并不会是表示噪声点的-1)，也可也这样理解，这些点不适合做核心点，但是会被包含在某个核心点的范围之内</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">core_samples_mask[db.core_sample_indices_] = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##每个数据的分类</span></span><br><span class="line">lables = db.labels_</span><br><span class="line"></span><br><span class="line"><span class="comment">##分类个数：lables中包含-1，表示噪声点</span></span><br><span class="line">n_clusters_ =len(np.unique(lables)) - (<span class="number">1</span> <span class="keyword">if</span> <span class="number">-1</span> <span class="keyword">in</span> lables <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">unique_labels = set(lables)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">   1)np.linspace 返回[0,1]之间的len(unique_labels) 个数</span></span><br><span class="line"><span class="string">   2)plt.cm 一个颜色映射模块</span></span><br><span class="line"><span class="string">   3)生成的每个colors包含4个值，分别是rgba</span></span><br><span class="line"><span class="string">   4)其实这行代码的意思就是生成4个可以和光谱对应的颜色值</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">colors = plt.cm.Spectral(np.linspace(<span class="number">0</span>, <span class="number">1</span>, len(unique_labels)))</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> zip(unique_labels, colors):</span><br><span class="line">    <span class="comment">##-1表示噪声点,这里的k表示黑色</span></span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">-1</span>:</span><br><span class="line">        col = <span class="string">'k'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">##生成一个True、False数组，lables == k 的设置成True</span></span><br><span class="line">    class_member_mask = (lables == k)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">##两个数组做&amp;运算，找出即是核心点又等于分类k的值  markeredgecolor='k',</span></span><br><span class="line">    xy = X[class_member_mask &amp; core_samples_mask]</span><br><span class="line">    plt.plot(xy[:, <span class="number">0</span>], xy[:, <span class="number">1</span>], <span class="string">'o'</span>, c=col,markersize=<span class="number">14</span>)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">       1)~优先级最高，按位对core_samples_mask 求反，求出的是噪音点的位置</span></span><br><span class="line"><span class="string">       2)&amp; 于运算之后，求出虽然刚开始是噪音点的位置，但是重新归类却属于k的点</span></span><br><span class="line"><span class="string">       3)对核心分类之后进行的扩展</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    xy = X[class_member_mask &amp; ~core_samples_mask]     </span><br><span class="line">    plt.plot(xy[:, <span class="number">0</span>], xy[:, <span class="number">1</span>], <span class="string">'o'</span>, c=col,markersize=<span class="number">6</span>)</span><br><span class="line">    </span><br><span class="line">plt.title(<span class="string">'Estimated number of clusters: %d'</span> % n_clusters_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="效果图-1"><a href="#效果图-1" class="headerlink" title="效果图"></a>效果图</h3><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/dbscan_2.png" class="">
</li>
</ol>
<p>如果不进行第二步中的扩展，所有的小圆点都应该是噪声点（不符合第一步核心点的要求）</p>
<h3 id="算法优缺点"><a href="#算法优缺点" class="headerlink" title="算法优缺点"></a>算法优缺点</h3><ol>
<li>优点<br> a. 可以发现任意形状的聚类</li>
<li>缺点<br> a. 随着数据量的增加，对I/O、内存的要求也随之增加。<br> b. 如果密度分布不均匀，聚类效果较差</li>
</ol>
<h2 id="Birch"><a href="#Birch" class="headerlink" title="Birch"></a>Birch</h2><h3 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h3><p>Birch(利用层次方法的平衡迭代规约和聚类)：就是通过聚类特征(CF)形成一个聚类特征树，root层的CF个数就是聚类个数。</p>
<h3 id="相关概念："><a href="#相关概念：" class="headerlink" title="相关概念："></a>相关概念：</h3><p>聚类特征(CF)：每一个CF是一个三元组,可以用（N，LS，SS）表示.其中N代表了这个CF中拥有的样本点的数量;LS代表了这个CF中拥有的样本点各特征维度的和向量,SS代表了这个CF中拥有的样本点各特征维度的平方和。</p>
<img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/birch_1.png" class="">
<p>如上图所示：N = 5<br>LS=(3+2+4+4+3,4+6+5+7+8)=(16,30)<br>SS =(32+22+42+42+32,42+62+52+72+82)=(54,190)</p>
<h3 id="图解过程-4"><a href="#图解过程-4" class="headerlink" title="图解过程"></a>图解过程</h3><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/birch_2.png" class="">
<p>对于上图中的CF Tree,限定了B=7,L=5， 也就是说内部节点最多有7个CF(CF90下的圆),而叶子节点最多有5个CF(CF90到CF94)。叶子节点是通过双向链表连通的。</p>
<h3 id="Birch算法函数"><a href="#Birch算法函数" class="headerlink" title="Birch算法函数"></a>Birch算法函数</h3><ol>
<li>sklearn.cluster.Birch</li>
<li>主要参数（<a href="http://scikit-learn.org/dev/modules/generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch" target="_blank" rel="noopener">详细参数</a>）<br> a. n_clusters ：聚类的目标个数。（可选）<br> b. threshold ：扫描半径（个人理解，官方说法比较绕口），设置小了分类就多。<br> c. branches_factor：每个节点中CF子集群的最大数量,默认为50。</li>
<li>主要属性<br>labels_ ：每个数据点的分类</li>
</ol>
<h3 id="算法示例：代码中有详细讲解内容-1"><a href="#算法示例：代码中有详细讲解内容-1" class="headerlink" title="算法示例：代码中有详细讲解内容"></a>算法示例：代码中有详细讲解内容</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> Birch</span><br><span class="line"></span><br><span class="line"><span class="comment"># X为样本特征，Y为样本簇类别， 共1000个样本，每个样本2个特征，共4个簇，簇中心在[-1,-1], [0,0],[1,1], [2,2]</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">1000</span>, n_features=<span class="number">2</span>, centers=[[<span class="number">-1</span>,<span class="number">-1</span>], [<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">2</span>]], cluster_std=[<span class="number">0.4</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>], </span><br><span class="line">                  random_state =<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##设置birch函数</span></span><br><span class="line">birch = Birch(n_clusters = <span class="keyword">None</span>)</span><br><span class="line"><span class="comment">##训练数据</span></span><br><span class="line">y_pred = birch.fit_predict(X)</span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="效果图：分别为n-clusters-None-和n-clusters-4"><a href="#效果图：分别为n-clusters-None-和n-clusters-4" class="headerlink" title="效果图：分别为n_clusters = None 和n_clusters = 4"></a>效果图：分别为n_clusters = None 和n_clusters = 4</h3><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/birch_3.png" class="">
<h2 id="GaussianMixtureModel-补"><a href="#GaussianMixtureModel-补" class="headerlink" title="GaussianMixtureModel(补)"></a>GaussianMixtureModel(补)</h2><h3 id="概述-5"><a href="#概述-5" class="headerlink" title="概述"></a>概述</h3><p>正太分布也叫高斯分布，正太分布的概率密度曲线也叫高斯分布概率曲线。</p>
<p>GaussianMixtureModel(混合高斯模型，GMM)。</p>
<p>聚类算法大多数通过相似度来判断，而相似度又大多采用欧式距离长短作为衡量依据。而GMM采用了新的判断依据：概率，即通过属于某一类的概率大小来判断最终的归属类别。</p>
<p>GMM的基本思想就是：任意形状的概率分布都可以用多个高斯分布函数去近似，也就是说GMM就是有多个单高斯密度分布（Gaussian）组成的，每个Gaussian叫一个”Component”，这些”Component”线性加成在一起就组成了 GMM 的概率密度函数，也就是下面的函数。</p>
<h3 id="数学公式-1"><a href="#数学公式-1" class="headerlink" title="数学公式"></a>数学公式</h3><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/gm_1.png" class="">
<p>这里不讲公式的具体推导过程，也不实现具体算法。列出来公式只是方便理解下面的函数中为什么需要那些参数。</p>
<ol>
<li>K：模型的个数，即Component的个数（聚类的个数）</li>
<li>$\pi_k$为第k个高斯的权重</li>
<li>p(x|k)则为第k个高斯概率密度,其均值为μk,方差为σk</li>
<li>上述参数，除了K是直接给定之外，其他参数都是通过EM算法估算出来的。(有个参数是指定EM算法参数的)</li>
</ol>
<h3 id="GaussianMixtureModel-算法函数"><a href="#GaussianMixtureModel-算法函数" class="headerlink" title="GaussianMixtureModel 算法函数"></a>GaussianMixtureModel 算法函数</h3><ol>
<li>from sklearn.mixture.GaussianMixture</li>
<li>主要参数（<a href="http://scikit-learn.org/dev/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" target="_blank" rel="noopener">详细参数</a>）<br> a. n_components ：高斯模型的个数，即聚类的目标个数<br> b. covariance_type : 通过EM算法估算参数时使用的协方差类型，默认是”full”<pre><code> * full：每个模型使用自己的一般协方差矩阵
 * tied：所用模型共享一个一般协方差矩阵
 * diag：每个模型使用自己的对角线协方差矩阵
 * spherical：每个模型使用自己的单一方差
</code></pre></li>
</ol>
<h3 id="算法示例：代码中有详细讲解内容-2"><a href="#算法示例：代码中有详细讲解内容-2" class="headerlink" title="算法示例：代码中有详细讲解内容"></a>算法示例：代码中有详细讲解内容</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line"></span><br><span class="line"><span class="comment"># X为样本特征，Y为样本簇类别， 共1000个样本，每个样本2个特征，共4个簇，簇中心在[-1,-1], [0,0],[1,1], [2,2]</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">1000</span>, n_features=<span class="number">2</span>, centers=[[<span class="number">-1</span>,<span class="number">-1</span>], [<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">2</span>]], cluster_std=[<span class="number">0.4</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>], </span><br><span class="line">                  random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##设置gmm函数</span></span><br><span class="line">gmm = GaussianMixture(n_components=<span class="number">4</span>, covariance_type=<span class="string">'full'</span>).fit(X)</span><br><span class="line"><span class="comment">##训练数据</span></span><br><span class="line">y_pred = gmm.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="效果图-2"><a href="#效果图-2" class="headerlink" title="效果图"></a>效果图</h3><img src="/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/gm_2.png" class="">
<p>跟图15对比可以看出，虽然使用同样的数据，但是不同的算法的聚类效果是不一样的</p>

      
    </div>

    
      
      

  <div class="post-copyright">
    <p class="copyright-item">
      <span>原文作者: </span>
      <a href="http://code-monkey.top">Anthon</a>
    </p>
    <p class="copyright-item">
      <span>原文链接: </span>
      <a href="http://code-monkey.top/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/">http://code-monkey.top/2017/10/14/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/</a>
    </p>
    <p class="copyright-item">
      <span>许可协议: </span>
      
      <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>
    </p>
  </div>



      
      
    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/python/">python</a>
            
              <a href="/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/">聚类算法</a>
            
              <a href="/tags/kmeans/">kmeans</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2017/10/18/K%E8%BF%91%E9%82%BB%E6%B3%95-KNN-%E5%8E%9F%E7%90%86%E5%B0%8F%E7%BB%93/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">K近邻法(KNN)原理小结</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    
    
      <a class="next" href="/2017/09/20/%E8%A7%A3%E5%AF%86%E8%B0%B7%E6%AD%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/">
        <span class="next-text nav-default">解密谷歌机器学习工程最佳实践</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>


      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
  </div>


        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:tanghuaidong@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
        
          <a href="https://github.com/tangboy" target="_blank" rel="noopener" class="iconfont icon-github" title="github"></a>
        
      
    
      
    
      
        
          <a href="https://www.zhihu.com/people/tang-huai-dong/activities" target="_blank" rel="noopener" class="iconfont icon-zhihu" title="zhihu"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    
      
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>



<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even" target="_blank" rel="noopener">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2015 - 
    
    2020

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Anthon</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  
  

  



    
  



  
  





  
    <script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

  
    <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=2.10.1"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  </body>
</html>
