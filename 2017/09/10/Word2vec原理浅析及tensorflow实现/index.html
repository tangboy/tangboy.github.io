<!DOCTYPE html>
<html lang="zh-cn">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="Word2vec原理浅析及tensorflow实现">




  <meta name="keywords" content="Word2vec, tensorflow, Anthon">










  <link rel="alternate" href="/default" title="Anthon">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.10.1">



<link rel="canonical" href="http://code-monkey.top/2017/09/10/Word2vec原理浅析及tensorflow实现/">



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css">




  <link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css">



<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.10.1">



  



  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>









<script>
  window.config = {"leancloud":{"app_id":null,"app_key":null},"toc":true,"fancybox":true,"pjax":true};
</script>

    <title> Word2vec原理浅析及tensorflow实现 - Anthon </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/." class="logo">Anthon</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            标签
          
        </li>
      </a>
    
      <a href="/about">
        <li class="mobile-menu-item">
          
          
            关于
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Anthon</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/about">
            
            
              关于
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Word2vec原理浅析及tensorflow实现
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2017-09-10
        </span>
        
        
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Word2vec简介"><span class="toc-text">Word2vec简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word2vec详细实现"><span class="toc-text">Word2vec详细实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#神经网络结构"><span class="toc-text">神经网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#skip-gram和CBOW"><span class="toc-text">skip-gram和CBOW</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#负采样（Negative-Sampling）"><span class="toc-text">负采样（Negative Sampling）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensorflow实现"><span class="toc-text">Tensorflow实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Skip-Gram"><span class="toc-text">Skip-Gram</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CBOW"><span class="toc-text">CBOW</span></a></li></ol></li></ol></li></ol>
    </div>
  </div>



    <div class="post-content">
      
        <h2 id="Word2vec简介"><a href="#Word2vec简介" class="headerlink" title="Word2vec简介"></a>Word2vec简介</h2><p>Word2Vec是由Google的Mikolov等人提出的一个词向量计算模型。</p>
<ul>
<li>输入：大量已分词的文本</li>
<li>输出：用一个稠密向量来表示每个词</li>
</ul>
<p>词向量的重要意义在于将自然语言转换成了计算机能够理解的向量。相对于词袋模型、TF-IDF等模型，词向量能抓住词的上下文、语义，衡量词与词的相似性，在文本分类、情感分析等许多自然语言处理领域有重要作用。</p>
<h2 id="Word2vec详细实现"><a href="#Word2vec详细实现" class="headerlink" title="Word2vec详细实现"></a>Word2vec详细实现</h2><p>word2vec的详细实现，简而言之，就是一个三层的神经网络。要理解word2vec的实现，需要的预备知识是神经网络和Logistic Regression。</p>
<h3 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h3><img src="/2017/09/10/Word2vec原理浅析及tensorflow实现/1.png">
<p>上图是Word2vec的简要流程图。首先假设，词库里的词数为10000; 词向量的长度为300（根据<a href="https://www.youtube.com/playlist?list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG" target="_blank" rel="noopener">斯坦福CS224d</a>的讲解，词向量一般为25-1000维，300维是一个好的选择）。下面以单个训练样本为例，依次介绍每个部分的含义。</p>
<ol>
<li>输入层：输入为一个词的one-hot向量表示。这个向量长度为10000。假设这个词为ants，ants在词库中的ID为i，则输入向量的第i个分量为1，其余为0。[0, 0, …, 0, 0, 1, 0, 0, …, 0, 0]</li>
<li>隐藏层：隐藏层的神经元个数就是词向量的长度。隐藏层的参数是一个[10000 ，300]的矩阵。 <strong>实际上，这个参数矩阵就是词向量</strong>。回忆一下矩阵相乘，一个one-hot行向量和矩阵相乘，结果就是矩阵的第i行。经过隐藏层，实际上就是把10000维的one-hot向量映射成了最终想要得到的300维的词向量。</li>
</ol>
<img src="/2017/09/10/Word2vec原理浅析及tensorflow实现/2.png">
<ol>
<li>输出层: 输出层的神经元个数为总词数10000，参数矩阵尺寸为[300，10000]。词向量经过矩阵计算后再加上softmax归一化，重新变为10000维的向量，每一维对应词库中的一个词与输入的词（在这里是ants）共同出现在上下文中的概率。</li>
</ol>
<img src="/2017/09/10/Word2vec原理浅析及tensorflow实现/3.png">
<p>上图中计算了car与ants共现的概率，car所对应的300维列向量就是输出层参数矩阵中的一列。输出层的参数矩阵是[300，10000]，也就是计算了词库中所有词与ants共现的概率。输出层的参数矩阵在训练完毕后没有作用。</p>
<ol>
<li>训练：训练样本（x, y）有输入也有输出，我们知道哪个词实际上跟ants共现，因此y也是一个10000维的向量。损失函数跟Logistic Regression相似，是神经网络的最终输出向量和y的交叉熵（cross-entropy）。最后用随机梯度下降来求解。</li>
</ol>
<img src="/2017/09/10/Word2vec原理浅析及tensorflow实现/4.png">
<p>上述步骤是一个词作为输入和一个上下文中的词作为输出的情况，但实际情况显然更复杂，什么是上下文呢？用一个词去预测周围的其他词，还是用周围的好多词来预测一个词？这里就要引入实际训练时的两个模型skip-gram和CBOW。<br><a id="more"></a></p>
<h3 id="skip-gram和CBOW"><a href="#skip-gram和CBOW" class="headerlink" title="skip-gram和CBOW"></a>skip-gram和CBOW</h3><ul>
<li>skip-gram： 核心思想是根据中心词来预测周围的词。假设中心词是cat，窗口长度为2，则根据cat预测左边两个词和右边两个词。这时，cat作为神经网络的input，预测的词作为label。下图为一个例子：</li>
</ul>
<img src="/2017/09/10/Word2vec原理浅析及tensorflow实现/5.png">
<p>在这里窗口长度为2，中心词一个一个移动，遍历所有文本。每一次中心词的移动，最多会产生4对训练样本（input，label）。</p>
<ul>
<li>CBOW（continuous-bag-of-words）：如果理解了skip-gram，那CBOW模型其实就是倒过来，用周围的所有词来预测中心词。这时候，每一次中心词的移动，只能产生一个训练样本。如果还是用上面的例子，则CBOW模型会产生下列4个训练样本：<ol>
<li>([quick, brown], the)</li>
<li>([the, brown, fox], quick)</li>
<li>([the, quick, fox, jumps], brown)</li>
<li>([quick, brown, jumps, over], fox)<br>这时候，input很可能是4个词，label只是一个词，怎么办呢？其实很简单，只要求平均就行了。经过隐藏层后，输入的4个词被映射成了4个300维的向量，对这4个向量求平均，然后就可以作为下一层的输入了。</li>
</ol>
</li>
</ul>
<p>两个模型相比，skip-gram模型能产生更多训练样本，抓住更多词与词之间语义上的细节，在语料足够多足够好的理想条件下，skip-gram模型是优于CBOW模型的。在语料较少的情况下，难以抓住足够多词与词之间的细节，CBOW模型求平均的特性，反而效果可能更好。</p>
<h3 id="负采样（Negative-Sampling）"><a href="#负采样（Negative-Sampling）" class="headerlink" title="负采样（Negative Sampling）"></a>负采样（Negative Sampling）</h3><p>实际训练时，还是假设词库有10000个词，词向量300维，那么每一层神经网络的参数是300万个，输出层相当于有一万个可能类的多分类问题。可以想象，这样的计算量非常非常非常大。<br>作者Mikolov等人提出了许多优化的方法，在这里着重讲一下负采样。<br>负采样的思想非常简单，简单地令人发指：我们知道最终神经网络经过softmax输出一个向量，只有一个概率最大的对应正确的单词，其余的称为negative sample。现在只选择5个negative sample，所以输出向量就只是一个6维的向量。要考虑的参数不是300万个，而减少到了1800个！ 这样做看上去很偷懒，实际效果却很好，大大提升了运算效率。<br>我们知道，训练神经网络时，每一次训练会对神经网络的参数进行微小的修改。在word2vec中，每一个训练样本并不会对所有参数进行修改。假设输入的词是cat，我们的隐藏层参数有300万个，但这一步训练只会修改cat相对应的300个参数，因为此时隐藏层的输出只跟这300个参数有关！<br>负采样是有效的，我们不需要那么多negative sample。Mikolov等人在论文中说：对于小数据集，负采样的个数在5-20个；对于大数据集，负采样的个数在2-5个。</p>
<p>那具体如何选择负采样的词呢？论文给出了如下公式：</p>
<img src="/2017/09/10/Word2vec原理浅析及tensorflow实现/6.png">
<p>其中f(w)是词频。可以看到，负采样的选择只跟词频有关，词频越大，越有可能选中。</p>
<h3 id="Tensorflow实现"><a href="#Tensorflow实现" class="headerlink" title="Tensorflow实现"></a>Tensorflow实现</h3><p>最后用tensorflow动手实践一下</p>
<p>这里只是训练了128维的词向量，并通过<a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" target="_blank" rel="noopener">TSNE</a>的方法可视化。作为练手和深入理解word2vec不错，实战还是推荐gensim。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># These are all the modules we'll be using later. Make sure you can import them</span></span><br><span class="line"><span class="comment"># before proceeding further.</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pylab</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> range</span><br><span class="line"><span class="keyword">from</span> six.moves.urllib.request <span class="keyword">import</span> urlretrieve</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br></pre></td></tr></table></figure>
<p>Download the data from the source website if necessary.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'http://mattmahoney.net/dc/'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybe_download</span><span class="params">(filename, expected_bytes)</span>:</span></span><br><span class="line">  <span class="string">"""Download a file if not present, and make sure it's the right size."""</span></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filename):</span><br><span class="line">    filename, _ = urlretrieve(url + filename, filename)</span><br><span class="line">  statinfo = os.stat(filename)</span><br><span class="line">  <span class="keyword">if</span> statinfo.st_size == expected_bytes:</span><br><span class="line">    print(<span class="string">'Found and verified %s'</span> % filename)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(statinfo.st_size)</span><br><span class="line">    <span class="keyword">raise</span> Exception(</span><br><span class="line">      <span class="string">'Failed to verify '</span> + filename + <span class="string">'. Can you get to it with a browser?'</span>)</span><br><span class="line">  <span class="keyword">return</span> filename</span><br><span class="line"></span><br><span class="line">filename = maybe_download(<span class="string">'text8.zip'</span>, <span class="number">31344016</span>)</span><br></pre></td></tr></table></figure>
<p>Read the data into a string.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(filename)</span>:</span></span><br><span class="line">  <span class="string">"""Extract the first file enclosed in a zip file as a list of words"""</span></span><br><span class="line">  <span class="keyword">with</span> zipfile.ZipFile(filename) <span class="keyword">as</span> f:</span><br><span class="line">    data = tf.compat.as_str(f.read(f.namelist()[<span class="number">0</span>])).split()</span><br><span class="line">  <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">words = read_data(filename)</span><br><span class="line">print(<span class="string">'Data size %d'</span> % len(words))</span><br></pre></td></tr></table></figure>
<p>Build the dictionary and replace rare words with UNK token.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">vocabulary_size = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(words)</span>:</span></span><br><span class="line">  count = [[<span class="string">'UNK'</span>, <span class="number">-1</span>]]</span><br><span class="line">  count.extend(collections.Counter(words).most_common(vocabulary_size - <span class="number">1</span>))</span><br><span class="line">  dictionary = dict()</span><br><span class="line">  <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</span><br><span class="line">    dictionary[word] = len(dictionary)</span><br><span class="line">  data = list()</span><br><span class="line">  unk_count = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</span><br><span class="line">      index = dictionary[word]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      index = <span class="number">0</span>  <span class="comment"># dictionary['UNK']</span></span><br><span class="line">      unk_count = unk_count + <span class="number">1</span></span><br><span class="line">    data.append(index)</span><br><span class="line">  count[<span class="number">0</span>][<span class="number">1</span>] = unk_count</span><br><span class="line">  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</span><br><span class="line">  <span class="keyword">return</span> data, count, dictionary, reverse_dictionary</span><br><span class="line"></span><br><span class="line">data, count, dictionary, reverse_dictionary = build_dataset(words)</span><br><span class="line">print(<span class="string">'Most common words (+UNK)'</span>, count[:<span class="number">5</span>])</span><br><span class="line">print(<span class="string">'Sample data'</span>, data[:<span class="number">10</span>])</span><br><span class="line"><span class="keyword">del</span> words  <span class="comment"># Hint to reduce memory.</span></span><br></pre></td></tr></table></figure>
<p>Function to generate a training batch for the skip-gram model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">data_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(batch_size, num_skips, skip_window)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> data_index</span><br><span class="line">    <span class="keyword">assert</span> batch_size % num_skips == <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> num_skips &lt;= <span class="number">2</span> * skip_window</span><br><span class="line">    batch = np.ndarray(shape=(batch_size), dtype=np.int32)</span><br><span class="line">    labels = np.ndarray(shape=(batch_size, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line">    span = <span class="number">2</span> * skip_window + <span class="number">1</span> <span class="comment"># [ skip_window target skip_window ]</span></span><br><span class="line">    buffer = collections.deque(maxlen=span)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(span):</span><br><span class="line">        buffer.append(data[data_index])</span><br><span class="line">        data_index = (data_index + <span class="number">1</span>) % len(data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size // num_skips):</span><br><span class="line">        target = skip_window  <span class="comment"># target label at the center of the buffer</span></span><br><span class="line">        targets_to_avoid = [ skip_window ]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_skips):</span><br><span class="line">            <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">                target = random.randint(<span class="number">0</span>, span - <span class="number">1</span>)</span><br><span class="line">            targets_to_avoid.append(target)</span><br><span class="line">            batch[i * num_skips + j] = buffer[skip_window]</span><br><span class="line">            labels[i * num_skips + j, <span class="number">0</span>] = buffer[target]</span><br><span class="line">        buffer.append(data[data_index])</span><br><span class="line">        data_index = (data_index + <span class="number">1</span>) % len(data)</span><br><span class="line">    <span class="keyword">return</span> batch, labels</span><br><span class="line"></span><br><span class="line">print(<span class="string">'data:'</span>, [reverse_dictionary[di] <span class="keyword">for</span> di <span class="keyword">in</span> data[:<span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> num_skips, skip_window <span class="keyword">in</span> [(<span class="number">2</span>, <span class="number">1</span>), (<span class="number">4</span>, <span class="number">2</span>)]:</span><br><span class="line">    data_index = <span class="number">0</span></span><br><span class="line">    batch, labels = generate_batch(batch_size=<span class="number">8</span>, num_skips=num_skips, skip_window=skip_window)</span><br><span class="line">    print(<span class="string">'\nwith num_skips = %d and skip_window = %d:'</span> % (num_skips, skip_window))</span><br><span class="line">    print(<span class="string">'    batch:'</span>, [reverse_dictionary[bi] <span class="keyword">for</span> bi <span class="keyword">in</span> batch])</span><br><span class="line">    print(<span class="string">'    labels:'</span>, [reverse_dictionary[li] <span class="keyword">for</span> li <span class="keyword">in</span> labels.reshape(<span class="number">8</span>)])</span><br></pre></td></tr></table></figure>
<h4 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h4><p>Train a skip-gram model.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">embedding_size = <span class="number">128</span> <span class="comment"># Dimension of the embedding vector.</span></span><br><span class="line">skip_window = <span class="number">1</span> <span class="comment"># How many words to consider left and right.</span></span><br><span class="line">num_skips = <span class="number">2</span> <span class="comment"># How many times to reuse an input to generate a label.</span></span><br><span class="line"><span class="comment"># We pick a random validation set to sample nearest neighbors. here we limit the</span></span><br><span class="line"><span class="comment"># validation samples to the words that have a low numeric ID, which by</span></span><br><span class="line"><span class="comment"># construction are also the most frequent.</span></span><br><span class="line">valid_size = <span class="number">16</span> <span class="comment"># Random set of words to evaluate similarity on.</span></span><br><span class="line">valid_window = <span class="number">100</span> <span class="comment"># Only pick dev samples in the head of the distribution.</span></span><br><span class="line">valid_examples = np.array(random.sample(range(valid_window), valid_size))</span><br><span class="line"></span><br><span class="line"><span class="comment">#######important#########</span></span><br><span class="line">num_sampled = <span class="number">64</span> <span class="comment"># Number of negative examples to sample.</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default(), tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Input data.</span></span><br><span class="line">  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])</span><br><span class="line">  train_labels = tf.placeholder(tf.int32, shape=[batch_size, <span class="number">1</span>])</span><br><span class="line">  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Variables.</span></span><br><span class="line">  embeddings = tf.Variable(</span><br><span class="line">    tf.random_uniform([vocabulary_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">  softmax_weights = tf.Variable(</span><br><span class="line">    tf.truncated_normal([vocabulary_size, embedding_size],</span><br><span class="line">                         stddev=<span class="number">1.0</span> / math.sqrt(embedding_size)))</span><br><span class="line">  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Model.</span></span><br><span class="line">  <span class="comment"># Look up embeddings for inputs.</span></span><br><span class="line">  embed = tf.nn.embedding_lookup(embeddings, train_dataset)</span><br><span class="line">  <span class="comment"># Compute the softmax loss, using a sample of the negative labels each time.</span></span><br><span class="line">  loss = tf.reduce_mean(</span><br><span class="line">    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,</span><br><span class="line">                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Optimizer.</span></span><br><span class="line">  <span class="comment"># <span class="doctag">Note:</span> The optimizer will optimize the softmax_weights AND the embeddings.</span></span><br><span class="line">  <span class="comment"># This is because the embeddings are defined as a variable quantity and the</span></span><br><span class="line">  <span class="comment"># optimizer's `minimize` method will by default modify all variable quantities</span></span><br><span class="line">  <span class="comment"># that contribute to the tensor it is passed.</span></span><br><span class="line">  <span class="comment"># See docs on `tf.train.Optimizer.minimize()` for more details.</span></span><br><span class="line">  optimizer = tf.train.AdagradOptimizer(<span class="number">1.0</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Compute the similarity between minibatch examples and all embeddings.</span></span><br><span class="line">  <span class="comment"># We use the cosine distance:</span></span><br><span class="line">  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), <span class="number">1</span>, keep_dims=<span class="keyword">True</span>))</span><br><span class="line">  normalized_embeddings = embeddings / norm</span><br><span class="line">  valid_embeddings = tf.nn.embedding_lookup(</span><br><span class="line">    normalized_embeddings, valid_dataset)</span><br><span class="line">  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">num_steps = <span class="number">100001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">  tf.global_variables_initializer().run()</span><br><span class="line">  print(<span class="string">'Initialized'</span>)</span><br><span class="line">  average_loss = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">    batch_data, batch_labels = generate_batch(</span><br><span class="line">      batch_size, num_skips, skip_window)</span><br><span class="line">    feed_dict = &#123;train_dataset : batch_data, train_labels : batch_labels&#125;</span><br><span class="line">    _, l = session.run([optimizer, loss], feed_dict=feed_dict)</span><br><span class="line">    average_loss += l</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">      <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">        average_loss = average_loss / <span class="number">2000</span></span><br><span class="line">      <span class="comment"># The average loss is an estimate of the loss over the last 2000 batches.</span></span><br><span class="line">      print(<span class="string">'Average loss at step %d: %f'</span> % (step, average_loss))</span><br><span class="line">      average_loss = <span class="number">0</span></span><br><span class="line">    <span class="comment"># note that this is expensive (~20% slowdown if computed every 500 steps)</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">      sim = similarity.eval()</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(valid_size):</span><br><span class="line">        valid_word = reverse_dictionary[valid_examples[i]]</span><br><span class="line">        top_k = <span class="number">8</span> <span class="comment"># number of nearest neighbors</span></span><br><span class="line">        nearest = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k+<span class="number">1</span>]</span><br><span class="line">        log = <span class="string">'Nearest to %s:'</span> % valid_word</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(top_k):</span><br><span class="line">          close_word = reverse_dictionary[nearest[k]]</span><br><span class="line">          log = <span class="string">'%s %s,'</span> % (log, close_word)</span><br><span class="line">        print(log)</span><br><span class="line">  final_embeddings = normalized_embeddings.eval()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">num_points = <span class="number">400</span></span><br><span class="line"></span><br><span class="line">tsne = TSNE(perplexity=<span class="number">30</span>, n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, n_iter=<span class="number">5000</span>)</span><br><span class="line">two_d_embeddings = tsne.fit_transform(final_embeddings[<span class="number">1</span>:num_points+<span class="number">1</span>, :])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(embeddings, labels)</span>:</span></span><br><span class="line">  <span class="keyword">assert</span> embeddings.shape[<span class="number">0</span>] &gt;= len(labels), <span class="string">'More labels than embeddings'</span></span><br><span class="line">  pylab.figure(figsize=(<span class="number">15</span>,<span class="number">15</span>))  <span class="comment"># in inches</span></span><br><span class="line">  <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">    x, y = embeddings[i,:]</span><br><span class="line">    pylab.scatter(x, y)</span><br><span class="line">    pylab.annotate(label, xy=(x, y), xytext=(<span class="number">5</span>, <span class="number">2</span>), textcoords=<span class="string">'offset points'</span>,</span><br><span class="line">                   ha=<span class="string">'right'</span>, va=<span class="string">'bottom'</span>)</span><br><span class="line">  pylab.show()</span><br><span class="line"></span><br><span class="line">words = [reverse_dictionary[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_points+<span class="number">1</span>)]</span><br><span class="line">plot(two_d_embeddings, words)</span><br></pre></td></tr></table></figure>
<img src="/2017/09/10/Word2vec原理浅析及tensorflow实现/7.png">
<h4 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">data_index_cbow = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cbow_batch</span><span class="params">(batch_size, num_skips, skip_window)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> data_index_cbow</span><br><span class="line">    <span class="keyword">assert</span> batch_size % num_skips == <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> num_skips &lt;= <span class="number">2</span> * skip_window</span><br><span class="line">    batch = np.ndarray(shape=(batch_size), dtype=np.int32)</span><br><span class="line">    labels = np.ndarray(shape=(batch_size, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line">    span = <span class="number">2</span> * skip_window + <span class="number">1</span> <span class="comment"># [ skip_window target skip_window ]</span></span><br><span class="line">    buffer = collections.deque(maxlen=span)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(span):</span><br><span class="line">        buffer.append(data[data_index_cbow])</span><br><span class="line">        data_index_cbow = (data_index_cbow + <span class="number">1</span>) % len(data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size // num_skips):</span><br><span class="line">        target = skip_window  <span class="comment"># target label at the center of the buffer</span></span><br><span class="line">        targets_to_avoid = [ skip_window ]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_skips):</span><br><span class="line">            <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">                target = random.randint(<span class="number">0</span>, span - <span class="number">1</span>)</span><br><span class="line">            targets_to_avoid.append(target)</span><br><span class="line">            batch[i * num_skips + j] = buffer[skip_window]</span><br><span class="line">            labels[i * num_skips + j, <span class="number">0</span>] = buffer[target]</span><br><span class="line">        buffer.append(data[data_index_cbow])</span><br><span class="line">        data_index_cbow = (data_index_cbow + <span class="number">1</span>) % len(data)</span><br><span class="line">    cbow_batch = np.ndarray(shape=(batch_size), dtype=np.int32)</span><br><span class="line">    cbow_labels = np.ndarray(shape=(batch_size // (skip_window * <span class="number">2</span>), <span class="number">1</span>), dtype=np.int32)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        cbow_batch[i] = labels[i]</span><br><span class="line">    cbow_batch = np.reshape(cbow_batch, [batch_size // (skip_window * <span class="number">2</span>), skip_window * <span class="number">2</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size // (skip_window * <span class="number">2</span>)):</span><br><span class="line">        <span class="comment"># center word</span></span><br><span class="line">        cbow_labels[i] = batch[<span class="number">2</span> * skip_window * i]</span><br><span class="line">    <span class="keyword">return</span> cbow_batch, cbow_labels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># actual batch_size = batch_size // (2 * skip_window)</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">embedding_size = <span class="number">128</span> <span class="comment"># Dimension of the embedding vector.</span></span><br><span class="line">skip_window = <span class="number">1</span> <span class="comment"># How many words to consider left and right.</span></span><br><span class="line">num_skips = <span class="number">2</span> <span class="comment"># How many times to reuse an input to generate a label.</span></span><br><span class="line"><span class="comment"># We pick a random validation set to sample nearest neighbors. here we limit the</span></span><br><span class="line"><span class="comment"># validation samples to the words that have a low numeric ID, which by</span></span><br><span class="line"><span class="comment"># construction are also the most frequent.</span></span><br><span class="line">valid_size = <span class="number">16</span> <span class="comment"># Random set of words to evaluate similarity on.</span></span><br><span class="line">valid_window = <span class="number">100</span> <span class="comment"># Only pick dev samples in the head of the distribution.</span></span><br><span class="line">valid_examples = np.array(random.sample(range(valid_window), valid_size))</span><br><span class="line"></span><br><span class="line"><span class="comment">#######important#########</span></span><br><span class="line">num_sampled = <span class="number">64</span> <span class="comment"># Number of negative examples to sample.</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default(), tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Input data.</span></span><br><span class="line">    train_dataset = tf.placeholder(tf.int32, shape=[batch_size // (skip_window * <span class="number">2</span>), skip_window * <span class="number">2</span>])</span><br><span class="line">    train_labels = tf.placeholder(tf.int32, shape=[batch_size // (skip_window * <span class="number">2</span>), <span class="number">1</span>])</span><br><span class="line">    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Variables.</span></span><br><span class="line">    embeddings = tf.Variable(</span><br><span class="line">      tf.random_uniform([vocabulary_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">    softmax_weights = tf.Variable(</span><br><span class="line">      tf.truncated_normal([vocabulary_size, embedding_size],</span><br><span class="line">                         stddev=<span class="number">1.0</span> / math.sqrt(embedding_size)))</span><br><span class="line">    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Model.</span></span><br><span class="line">  <span class="comment"># Look up embeddings for inputs.</span></span><br><span class="line">    embed = tf.nn.embedding_lookup(embeddings, train_dataset)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># reshape embed</span></span><br><span class="line">    embed = tf.reshape(embed, (skip_window * <span class="number">2</span>, batch_size // (skip_window * <span class="number">2</span>), embedding_size))</span><br><span class="line">    <span class="comment"># average embed</span></span><br><span class="line">    embed = tf.reduce_mean(embed, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Compute the softmax loss, using a sample of the negative labels each time.</span></span><br><span class="line">    loss = tf.reduce_mean(</span><br><span class="line">      tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,</span><br><span class="line">                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Optimizer.</span></span><br><span class="line">  <span class="comment"># <span class="doctag">Note:</span> The optimizer will optimize the softmax_weights AND the embeddings.</span></span><br><span class="line">  <span class="comment"># This is because the embeddings are defined as a variable quantity and the</span></span><br><span class="line">  <span class="comment"># optimizer's `minimize` method will by default modify all variable quantities</span></span><br><span class="line">  <span class="comment"># that contribute to the tensor it is passed.</span></span><br><span class="line">  <span class="comment"># See docs on `tf.train.Optimizer.minimize()` for more details.</span></span><br><span class="line">    optimizer = tf.train.AdagradOptimizer(<span class="number">1.0</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Compute the similarity between minibatch examples and all embeddings.</span></span><br><span class="line">  <span class="comment"># We use the cosine distance:</span></span><br><span class="line">    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), <span class="number">1</span>, keep_dims=<span class="keyword">True</span>))</span><br><span class="line">    normalized_embeddings = embeddings / norm</span><br><span class="line">    valid_embeddings = tf.nn.embedding_lookup(</span><br><span class="line">      normalized_embeddings, valid_dataset)</span><br><span class="line">    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">num_steps = <span class="number">100001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">  tf.global_variables_initializer().run()</span><br><span class="line">  print(<span class="string">'Initialized'</span>)</span><br><span class="line">  average_loss = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">    batch_data, batch_labels = get_cbow_batch(</span><br><span class="line">      batch_size, num_skips, skip_window)</span><br><span class="line">    feed_dict = &#123;train_dataset : batch_data, train_labels : batch_labels&#125;</span><br><span class="line">    _, l = session.run([optimizer, loss], feed_dict=feed_dict)</span><br><span class="line">    average_loss += l</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">      <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">        average_loss = average_loss / <span class="number">2000</span></span><br><span class="line">      <span class="comment"># The average loss is an estimate of the loss over the last 2000 batches.</span></span><br><span class="line">      print(<span class="string">'Average loss at step %d: %f'</span> % (step, average_loss))</span><br><span class="line">      average_loss = <span class="number">0</span></span><br><span class="line">    <span class="comment"># note that this is expensive (~20% slowdown if computed every 500 steps)</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">      sim = similarity.eval()</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(valid_size):</span><br><span class="line">        valid_word = reverse_dictionary[valid_examples[i]]</span><br><span class="line">        top_k = <span class="number">8</span> <span class="comment"># number of nearest neighbors</span></span><br><span class="line">        nearest = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k+<span class="number">1</span>]</span><br><span class="line">        log = <span class="string">'Nearest to %s:'</span> % valid_word</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(top_k):</span><br><span class="line">          close_word = reverse_dictionary[nearest[k]]</span><br><span class="line">          log = <span class="string">'%s %s,'</span> % (log, close_word)</span><br><span class="line">        print(log)</span><br><span class="line">  final_embeddings = normalized_embeddings.eval()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_points = <span class="number">400</span></span><br><span class="line"></span><br><span class="line">tsne = TSNE(perplexity=<span class="number">30</span>, n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, n_iter=<span class="number">5000</span>)</span><br><span class="line">two_d_embeddings = tsne.fit_transform(final_embeddings[<span class="number">1</span>:num_points+<span class="number">1</span>, :])</span><br><span class="line">words = [reverse_dictionary[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>, num_points+<span class="number">1</span>)]</span><br><span class="line">plot(two_d_embeddings, words)</span><br></pre></td></tr></table></figure>
<img src="/2017/09/10/Word2vec原理浅析及tensorflow实现/8.png">

      
    </div>

    
      
      

  <div class="post-copyright">
    <p class="copyright-item">
      <span>原文作者: </span>
      <a href="http://code-monkey.top">Anthon</a>
    </p>
    <p class="copyright-item">
      <span>原文链接: </span>
      <a href="http://code-monkey.top/2017/09/10/Word2vec原理浅析及tensorflow实现/">http://code-monkey.top/2017/09/10/Word2vec原理浅析及tensorflow实现/</a>
    </p>
    <p class="copyright-item">
      <span>许可协议: </span>
      
      <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>
    </p>
  </div>



      
      
    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/Word2vec/">Word2vec</a>
            
              <a href="/tags/tensorflow/">tensorflow</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2017/09/20/解密谷歌机器学习工程最佳实践/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">解密谷歌机器学习工程最佳实践</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    
    
      <a class="next" href="/2017/09/07/一篇文章搞懂Python中的面向对象编程/">
        <span class="next-text nav-default">一篇文章搞懂Python中的面向对象编程</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>


      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
  </div>


        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:tanghuaidong@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
        
          <a href="https://github.com/tangboy" class="iconfont icon-github" title="github"></a>
        
      
    
      
    
      
        
          <a href="https://www.zhihu.com/people/tang-huai-dong/activities" class="iconfont icon-zhihu" title="zhihu"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    

    
      
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>



<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2015 - 
    
    2018

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Anthon</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  
  

  



    
  



  
  





  
    <script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

  
    <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=2.10.1"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

  </body>
</html>
